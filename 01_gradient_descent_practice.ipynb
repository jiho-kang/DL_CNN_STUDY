{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "01-gradient-descent-practice.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiho-kang/DL_CNN_Study/blob/main/01_gradient_descent_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n",
        "* 사이킷런에서 보스턴 주택 가격 데이터 세트를 로드하고 이를 DataFrame으로 생성"
      ],
      "metadata": {
        "id": "rcDmO24Fijur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "boston = load_boston()\n",
        "boston"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T01:59:18.500164Z",
          "iopub.execute_input": "2022-02-10T01:59:18.500995Z",
          "iopub.status.idle": "2022-02-10T01:59:18.517361Z",
          "shell.execute_reply.started": "2022-02-10T01:59:18.50094Z",
          "shell.execute_reply": "2022-02-10T01:59:18.516516Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YMsXa_dijut",
        "outputId": "2431e6f8-6cba-4c60-f4de-6da3bfe71d75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
              " 'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
              "         4.9800e+00],\n",
              "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
              "         9.1400e+00],\n",
              "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
              "         4.0300e+00],\n",
              "        ...,\n",
              "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "         5.6400e+00],\n",
              "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
              "         6.4800e+00],\n",
              "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
              "         7.8800e+00]]),\n",
              " 'data_module': 'sklearn.datasets.data',\n",
              " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
              "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
              " 'filename': 'boston_house_prices.csv',\n",
              " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
              "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
              "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
              "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
              "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
              "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
              "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
              "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
              "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
              "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
              "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
              "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
              "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
              "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
              "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
              "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
              "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
              "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
              "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
              "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
              "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
              "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
              "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
              "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
              "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
              "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
              "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
              "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
              "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
              "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
              "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
              "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
              "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
              "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
              "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
              "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
              "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
              "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
              "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
              "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
              "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
              "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
              "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
              "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
              "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
              "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "bostonDF['PRICE'] = boston.target\n",
        "print(bostonDF.shape)\n",
        "bostonDF.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T01:59:37.944519Z",
          "iopub.execute_input": "2022-02-10T01:59:37.944994Z",
          "iopub.status.idle": "2022-02-10T01:59:37.968698Z",
          "shell.execute_reply.started": "2022-02-10T01:59:37.944953Z",
          "shell.execute_reply": "2022-02-10T01:59:37.967717Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "J91N1ipJijuu",
        "outputId": "efc76ffe-4d3a-4c3a-e9c6-b5fb91f74591"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 14)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f30b285c-9f53-4f46-9b90-a3e0225c40fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f30b285c-9f53-4f46-9b90-a3e0225c40fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f30b285c-9f53-4f46-9b90-a3e0225c40fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f30b285c-9f53-4f46-9b90-a3e0225c40fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  PRICE\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98   24.0\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14   21.6\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03   34.7\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94   33.4\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33   36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n",
        "* w1은 RM(방의 계수) 피처의 Weight 값\n",
        "* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n",
        "* bias는 Bias\n",
        "* N은 입력 데이터 건수\n",
        "![](https://raw.githubusercontent.com/chulminkw/CNN_PG/main/utils/images/Weight_update.png)\n"
      ],
      "metadata": {
        "id": "CrNlfY_Qijuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate = 0.01):\n",
        "    n = len(target)\n",
        "    predicted = (w1*rm) + (w2*lstat) + bias\n",
        "    diff = target - predicted # 실제값 - 예측값\n",
        "    bias_factors = np.ones((n,))\n",
        "    \n",
        "    w1_update = -(2/n) * learning_rate * np.dot(rm.T, diff) # np.dot을 함으로써 sum을 해줌\n",
        "    w2_update = -(2/n) * learning_rate * np.dot(lstat.T, diff)\n",
        "    bias_update = -(2/n) * learning_rate * np.dot(bias_factors.T, diff)\n",
        "    \n",
        "    mse_loss = np.mean(np.square(diff))\n",
        "    \n",
        "    return bias_update, w1_update, w2_update, mse_loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T05:26:31.695209Z",
          "iopub.execute_input": "2022-02-10T05:26:31.695624Z",
          "iopub.status.idle": "2022-02-10T05:26:31.703125Z",
          "shell.execute_reply.started": "2022-02-10T05:26:31.695592Z",
          "shell.execute_reply": "2022-02-10T05:26:31.702449Z"
        },
        "trusted": true,
        "id": "ATMWOvN2ijuv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수. \n",
        "# # rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 array가 다 입력됨. \n",
        "# # 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\n",
        "# def get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate=0.01):\n",
        "    \n",
        "#     # 데이터 건수\n",
        "#     N = len(target)\n",
        "#     # 예측 값. \n",
        "#     predicted = w1 * rm + w2*lstat + bias\n",
        "#     # 실제값과 예측값의 차이 \n",
        "#     diff = target - predicted\n",
        "#     # bias 를 array 기반으로 구하기 위해서 설정. \n",
        "#     bias_factors = np.ones((N,))\n",
        "    \n",
        "#     # weight와 bias를 얼마나 update할 것인지를 계산.  \n",
        "#     w1_update = -(2/N)*learning_rate*(np.dot(rm.T, diff))\n",
        "#     w2_update = -(2/N)*learning_rate*(np.dot(lstat.T, diff))\n",
        "#     bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff))\n",
        "    \n",
        "#     # Mean Squared Error값을 계산. \n",
        "#     mse_loss = np.mean(np.square(diff))\n",
        "    \n",
        "#     # weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 반환. \n",
        "#     return bias_update, w1_update, w2_update, mse_loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T05:26:31.839683Z",
          "iopub.execute_input": "2022-02-10T05:26:31.840105Z",
          "iopub.status.idle": "2022-02-10T05:26:31.843596Z",
          "shell.execute_reply.started": "2022-02-10T05:26:31.840058Z",
          "shell.execute_reply": "2022-02-10T05:26:31.843028Z"
        },
        "trusted": true,
        "id": "8QUpeuNQijuw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent 를 적용하는 함수 생성\n",
        "* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용. "
      ],
      "metadata": {
        "id": "-SSkr9hvijux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \n",
        "def gradient_descent(features, target, iter_epochs=1000, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.zeros((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "    \n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0]\n",
        "    lstat = features[:, 1]\n",
        "    \n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
        "    for i in range(iter_epochs):\n",
        "        # weight/bias update 값 계산 \n",
        "        bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate)\n",
        "        # weight/bias의 update 적용. \n",
        "        w1 = w1 - w1_update\n",
        "        w2 = w2 - w2_update\n",
        "        bias = bias - bias_update\n",
        "        if verbose:\n",
        "            if (i == 0) or ((i+1) % 100 == 0):\n",
        "                print('Epoch:', i+1,'/', iter_epochs)\n",
        "                print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', loss)\n",
        "                print()\n",
        "        \n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T05:26:32.129837Z",
          "iopub.execute_input": "2022-02-10T05:26:32.130152Z",
          "iopub.status.idle": "2022-02-10T05:26:32.138526Z",
          "shell.execute_reply.started": "2022-02-10T05:26:32.130118Z",
          "shell.execute_reply": "2022-02-10T05:26:32.137664Z"
        },
        "trusted": true,
        "id": "FeMUPEFvijux"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent 적용\n",
        "* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함. \n",
        "* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용."
      ],
      "metadata": {
        "id": "KvlVGixWijuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n",
        "scaled_features[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T05:26:32.771927Z",
          "iopub.execute_input": "2022-02-10T05:26:32.772797Z",
          "iopub.status.idle": "2022-02-10T05:26:32.783484Z",
          "shell.execute_reply.started": "2022-02-10T05:26:32.772746Z",
          "shell.execute_reply": "2022-02-10T05:26:32.782692Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_u4bTYFijuy",
        "outputId": "cc435ea8-729a-4d5c-9042-cffb2c99ea5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.57750527, 0.08967991],\n",
              "       [0.5479977 , 0.2044702 ],\n",
              "       [0.6943859 , 0.06346578],\n",
              "       [0.65855528, 0.03338852],\n",
              "       [0.68710481, 0.09933775]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, bias = gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T05:26:32.994608Z",
          "iopub.execute_input": "2022-02-10T05:26:32.995519Z",
          "iopub.status.idle": "2022-02-10T05:26:33.059916Z",
          "shell.execute_reply.started": "2022-02-10T05:26:32.995475Z",
          "shell.execute_reply": "2022-02-10T05:26:33.05935Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deAej_9-ijuz",
        "outputId": "c8fc7879-8b5c-443f-c6eb-2d1cb72b0cac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최초 w1, w2, bias: [0.] [0.] [0.]\n",
            "Epoch: 1 / 1000\n",
            "w1: [0.252369] w2: [0.10914761] bias: [0.45065613] loss: 592.1469169960474\n",
            "\n",
            "Epoch: 100 / 1000\n",
            "w1: [9.62405472] w2: [2.05283682] bias: [15.47316916] loss: 76.76427130847858\n",
            "\n",
            "Epoch: 200 / 1000\n",
            "w1: [11.5022803] w2: [-0.05484598] bias: [16.4532951] loss: 66.99833740927085\n",
            "\n",
            "Epoch: 300 / 1000\n",
            "w1: [12.78038167] w2: [-2.21908904] bias: [16.52264258] loss: 60.65897877390511\n",
            "\n",
            "Epoch: 400 / 1000\n",
            "w1: [13.91682547] w2: [-4.20115117] bias: [16.5330944] loss: 55.42734570065424\n",
            "\n",
            "Epoch: 500 / 1000\n",
            "w1: [14.94980404] w2: [-6.00297075] bias: [16.53824521] loss: 51.10427049189361\n",
            "\n",
            "Epoch: 600 / 1000\n",
            "w1: [15.89029715] w2: [-7.64003964] bias: [16.54161605] loss: 47.5319284654896\n",
            "\n",
            "Epoch: 700 / 1000\n",
            "w1: [16.74679738] w2: [-9.12730107] bias: [16.54357324] loss: 44.579932870966196\n",
            "\n",
            "Epoch: 800 / 1000\n",
            "w1: [17.52693159] w2: [-10.47839254] bias: [16.54427189] loss: 42.14054294794409\n",
            "\n",
            "Epoch: 900 / 1000\n",
            "w1: [18.23762719] w2: [-11.70571404] bias: [16.54384162] loss: 40.124730600391004\n",
            "\n",
            "Epoch: 1000 / 1000\n",
            "w1: [18.88518124] w2: [-12.82053747] bias: [16.54239941] loss: 38.458930326253615\n",
            "\n",
            "##### 최종 w1, w2, bias #######\n",
            "[18.88518124] [-12.82053747] [16.54239941]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 계산된 Weight와 Bias를 이용하여 Price 예측\n",
        "* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산. "
      ],
      "metadata": {
        "id": "icUc2ktsijuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "predicted[:15]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T02:25:39.753153Z",
          "iopub.execute_input": "2022-02-10T02:25:39.753542Z",
          "iopub.status.idle": "2022-02-10T02:25:39.759895Z",
          "shell.execute_reply.started": "2022-02-10T02:25:39.75351Z",
          "shell.execute_reply": "2022-02-10T02:25:39.759257Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmubJzmDijuz",
        "outputId": "e2b03dd4-4bc1-40f2-90f7-2d8f98c33066"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([26.29894642, 24.27001746, 28.84233748, 28.55127642, 28.24493494,\n",
              "       25.69289099, 21.62613759, 19.82778642, 14.05653949, 19.94509459,\n",
              "       20.10969008, 21.31811718, 20.02069898, 22.87338017, 22.69777096])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bostonDF['PREDICTED_PRICE'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T02:25:52.539337Z",
          "iopub.execute_input": "2022-02-10T02:25:52.539716Z",
          "iopub.status.idle": "2022-02-10T02:25:52.562529Z",
          "shell.execute_reply.started": "2022-02-10T02:25:52.539688Z",
          "shell.execute_reply": "2022-02-10T02:25:52.562056Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "onkBejqwiju0",
        "outputId": "f68fe89f-b6a9-4081-f89b-668b15921fb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-da51f2db-94b8-4233-acc4-647d969fcfbf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.298946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>24.270017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>28.842337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>28.551276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>28.244935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>25.692891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.626138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>19.827786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>14.056539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>19.945095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da51f2db-94b8-4233-acc4-647d969fcfbf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da51f2db-94b8-4233-acc4-647d969fcfbf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da51f2db-94b8-4233-acc4-647d969fcfbf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS  ...       B  LSTAT  PRICE  PREDICTED_PRICE\n",
              "0  0.00632  18.0   2.31   0.0  ...  396.90   4.98   24.0        26.298946\n",
              "1  0.02731   0.0   7.07   0.0  ...  396.90   9.14   21.6        24.270017\n",
              "2  0.02729   0.0   7.07   0.0  ...  392.83   4.03   34.7        28.842337\n",
              "3  0.03237   0.0   2.18   0.0  ...  394.63   2.94   33.4        28.551276\n",
              "4  0.06905   0.0   2.18   0.0  ...  396.90   5.33   36.2        28.244935\n",
              "5  0.02985   0.0   2.18   0.0  ...  394.12   5.21   28.7        25.692891\n",
              "6  0.08829  12.5   7.87   0.0  ...  395.60  12.43   22.9        21.626138\n",
              "7  0.14455  12.5   7.87   0.0  ...  396.90  19.15   27.1        19.827786\n",
              "8  0.21124  12.5   7.87   0.0  ...  386.63  29.93   16.5        14.056539\n",
              "9  0.17004  12.5   7.87   0.0  ...  386.71  17.10   18.9        19.945095\n",
              "\n",
              "[10 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras를 이용하여 보스턴 주택가격 모델 학습 및 예측\n",
        "* Dense Layer를 이용하여 퍼셉트론 구현. units는 1로 설정. "
      ],
      "metadata": {
        "id": "dw8Rc3uWiju0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    # units 설정 = 하나의 layer에 있는 node 갯수.\n",
        "    # input_shape = (feature 갯수, 차원)\n",
        "    # 회귀이므로 activation은 설정하지 않음. \n",
        "    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n",
        "    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n",
        "])\n",
        "# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\n",
        "model.fit(scaled_features, bostonDF['PRICE'].values, epochs=400)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T02:38:13.413402Z",
          "iopub.execute_input": "2022-02-10T02:38:13.413689Z",
          "iopub.status.idle": "2022-02-10T02:38:20.089917Z",
          "shell.execute_reply.started": "2022-02-10T02:38:13.413658Z",
          "shell.execute_reply": "2022-02-10T02:38:20.0894Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXt-Dd3Wiju0",
        "outputId": "19985d70-7ec0-44e4-bcdd-123adb5ec4ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "16/16 [==============================] - 1s 1ms/step - loss: 542.3615 - mse: 542.3615\n",
            "Epoch 2/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 530.1743 - mse: 530.1743\n",
            "Epoch 3/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 518.1933 - mse: 518.1933\n",
            "Epoch 4/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 506.5819 - mse: 506.5819\n",
            "Epoch 5/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 494.9981 - mse: 494.9981\n",
            "Epoch 6/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 483.8386 - mse: 483.8386\n",
            "Epoch 7/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 472.8990 - mse: 472.8990\n",
            "Epoch 8/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 462.1791 - mse: 462.1791\n",
            "Epoch 9/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 451.5965 - mse: 451.5965\n",
            "Epoch 10/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 441.3946 - mse: 441.3946\n",
            "Epoch 11/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 431.2924 - mse: 431.2924\n",
            "Epoch 12/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 421.5068 - mse: 421.5068\n",
            "Epoch 13/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 411.9192 - mse: 411.9192\n",
            "Epoch 14/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 402.5325 - mse: 402.5325\n",
            "Epoch 15/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 393.3439 - mse: 393.3439\n",
            "Epoch 16/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 384.4419 - mse: 384.4419\n",
            "Epoch 17/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 375.6220 - mse: 375.6220\n",
            "Epoch 18/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 367.1086 - mse: 367.1086\n",
            "Epoch 19/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 358.7853 - mse: 358.7853\n",
            "Epoch 20/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 350.6667 - mse: 350.6667\n",
            "Epoch 21/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 342.5958 - mse: 342.5958\n",
            "Epoch 22/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 334.9156 - mse: 334.9156\n",
            "Epoch 23/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 327.3238 - mse: 327.3238\n",
            "Epoch 24/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 319.9251 - mse: 319.9251\n",
            "Epoch 25/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 312.7218 - mse: 312.7218\n",
            "Epoch 26/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 305.6909 - mse: 305.6909\n",
            "Epoch 27/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 298.8819 - mse: 298.8819\n",
            "Epoch 28/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 292.1259 - mse: 292.1259\n",
            "Epoch 29/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 285.6241 - mse: 285.6241\n",
            "Epoch 30/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 279.3750 - mse: 279.3750\n",
            "Epoch 31/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 273.0968 - mse: 273.0968\n",
            "Epoch 32/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 267.1269 - mse: 267.1269\n",
            "Epoch 33/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 261.2366 - mse: 261.2366\n",
            "Epoch 34/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 255.5282 - mse: 255.5282\n",
            "Epoch 35/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 249.9754 - mse: 249.9754\n",
            "Epoch 36/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 244.5983 - mse: 244.5983\n",
            "Epoch 37/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 239.3020 - mse: 239.3020\n",
            "Epoch 38/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 234.1938 - mse: 234.1938\n",
            "Epoch 39/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 229.2339 - mse: 229.2339\n",
            "Epoch 40/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224.4124 - mse: 224.4124\n",
            "Epoch 41/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 219.7103 - mse: 219.7103\n",
            "Epoch 42/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 215.1394 - mse: 215.1394\n",
            "Epoch 43/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 210.7073 - mse: 210.7073\n",
            "Epoch 44/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 206.3977 - mse: 206.3977\n",
            "Epoch 45/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 202.2419 - mse: 202.2419\n",
            "Epoch 46/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 198.1553 - mse: 198.1553\n",
            "Epoch 47/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 194.2340 - mse: 194.2340\n",
            "Epoch 48/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 190.4268 - mse: 190.4268\n",
            "Epoch 49/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 186.6552 - mse: 186.6552\n",
            "Epoch 50/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 183.0696 - mse: 183.0696\n",
            "Epoch 51/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 179.6213 - mse: 179.6213\n",
            "Epoch 52/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 176.2257 - mse: 176.2257\n",
            "Epoch 53/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 172.9711 - mse: 172.9711\n",
            "Epoch 54/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 169.7426 - mse: 169.7426\n",
            "Epoch 55/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 166.6429 - mse: 166.6429\n",
            "Epoch 56/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 163.7523 - mse: 163.7523\n",
            "Epoch 57/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 160.8111 - mse: 160.8111\n",
            "Epoch 58/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 158.0407 - mse: 158.0407\n",
            "Epoch 59/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 155.2615 - mse: 155.2615\n",
            "Epoch 60/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 152.6971 - mse: 152.6971\n",
            "Epoch 61/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 150.1649 - mse: 150.1649\n",
            "Epoch 62/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 147.6810 - mse: 147.6810\n",
            "Epoch 63/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 145.3054 - mse: 145.3054\n",
            "Epoch 64/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 142.9903 - mse: 142.9903\n",
            "Epoch 65/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 140.7740 - mse: 140.7740\n",
            "Epoch 66/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 138.6303 - mse: 138.6303\n",
            "Epoch 67/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 136.5171 - mse: 136.5171\n",
            "Epoch 68/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 134.4922 - mse: 134.4922\n",
            "Epoch 69/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 132.6427 - mse: 132.6427\n",
            "Epoch 70/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 130.7063 - mse: 130.7063\n",
            "Epoch 71/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 128.9174 - mse: 128.9174\n",
            "Epoch 72/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 127.1272 - mse: 127.1272\n",
            "Epoch 73/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 125.4733 - mse: 125.4733\n",
            "Epoch 74/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 123.8008 - mse: 123.8008\n",
            "Epoch 75/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 122.2589 - mse: 122.2589\n",
            "Epoch 76/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 120.7175 - mse: 120.7175\n",
            "Epoch 77/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 119.2253 - mse: 119.2253\n",
            "Epoch 78/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 117.7966 - mse: 117.7966\n",
            "Epoch 79/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 116.3937 - mse: 116.3937\n",
            "Epoch 80/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 115.1406 - mse: 115.1406\n",
            "Epoch 81/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 113.8059 - mse: 113.8059\n",
            "Epoch 82/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 112.5470 - mse: 112.5470\n",
            "Epoch 83/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 111.3548 - mse: 111.3548\n",
            "Epoch 84/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 110.1574 - mse: 110.1574\n",
            "Epoch 85/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 109.0585 - mse: 109.0585\n",
            "Epoch 86/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 107.9790 - mse: 107.9790\n",
            "Epoch 87/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 106.8811 - mse: 106.8811\n",
            "Epoch 88/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 105.8801 - mse: 105.8801\n",
            "Epoch 89/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 104.8954 - mse: 104.8954\n",
            "Epoch 90/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 103.9213 - mse: 103.9213\n",
            "Epoch 91/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 102.9903 - mse: 102.9903\n",
            "Epoch 92/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 102.1327 - mse: 102.1327\n",
            "Epoch 93/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 101.2215 - mse: 101.2215\n",
            "Epoch 94/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 100.3468 - mse: 100.3468\n",
            "Epoch 95/400\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 99.5706 - mse: 99.5706\n",
            "Epoch 96/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 98.7505 - mse: 98.7505\n",
            "Epoch 97/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 98.0035 - mse: 98.0035\n",
            "Epoch 98/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 97.2186 - mse: 97.2186\n",
            "Epoch 99/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 96.4760 - mse: 96.4760\n",
            "Epoch 100/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 95.7638 - mse: 95.7638\n",
            "Epoch 101/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 95.0927 - mse: 95.0927\n",
            "Epoch 102/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 94.3738 - mse: 94.3738\n",
            "Epoch 103/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 93.7438 - mse: 93.7438\n",
            "Epoch 104/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 93.0787 - mse: 93.0787\n",
            "Epoch 105/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 92.4534 - mse: 92.4534\n",
            "Epoch 106/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 91.8450 - mse: 91.8450\n",
            "Epoch 107/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 91.2110 - mse: 91.2110\n",
            "Epoch 108/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 90.6269 - mse: 90.6269\n",
            "Epoch 109/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 90.0711 - mse: 90.0711\n",
            "Epoch 110/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 89.4809 - mse: 89.4809\n",
            "Epoch 111/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 88.9298 - mse: 88.9298\n",
            "Epoch 112/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 88.3968 - mse: 88.3968\n",
            "Epoch 113/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 87.8589 - mse: 87.8589\n",
            "Epoch 114/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 87.3507 - mse: 87.3507\n",
            "Epoch 115/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 86.8092 - mse: 86.8092\n",
            "Epoch 116/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 86.3181 - mse: 86.3181\n",
            "Epoch 117/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 85.8064 - mse: 85.8064\n",
            "Epoch 118/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 85.3262 - mse: 85.3262\n",
            "Epoch 119/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 84.8536 - mse: 84.8536\n",
            "Epoch 120/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 84.3509 - mse: 84.3509\n",
            "Epoch 121/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 83.8774 - mse: 83.8774\n",
            "Epoch 122/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 83.4283 - mse: 83.4283\n",
            "Epoch 123/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 82.9590 - mse: 82.9590\n",
            "Epoch 124/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 82.5302 - mse: 82.5302\n",
            "Epoch 125/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 82.0554 - mse: 82.0554\n",
            "Epoch 126/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 81.6252 - mse: 81.6252\n",
            "Epoch 127/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 81.1740 - mse: 81.1740\n",
            "Epoch 128/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 80.7579 - mse: 80.7579\n",
            "Epoch 129/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 80.3056 - mse: 80.3056\n",
            "Epoch 130/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 79.8912 - mse: 79.8912\n",
            "Epoch 131/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 79.4695 - mse: 79.4695\n",
            "Epoch 132/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 79.0416 - mse: 79.0416\n",
            "Epoch 133/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 78.6378 - mse: 78.6378\n",
            "Epoch 134/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 78.2256 - mse: 78.2256\n",
            "Epoch 135/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 77.8143 - mse: 77.8143\n",
            "Epoch 136/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 77.4136 - mse: 77.4136\n",
            "Epoch 137/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 77.0141 - mse: 77.0141\n",
            "Epoch 138/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 76.6165 - mse: 76.6165\n",
            "Epoch 139/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 76.2279 - mse: 76.2279\n",
            "Epoch 140/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 75.8182 - mse: 75.8182\n",
            "Epoch 141/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 75.4335 - mse: 75.4335\n",
            "Epoch 142/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 75.0599 - mse: 75.0599\n",
            "Epoch 143/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 74.6598 - mse: 74.6598\n",
            "Epoch 144/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 74.2824 - mse: 74.2824\n",
            "Epoch 145/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 73.9133 - mse: 73.9133\n",
            "Epoch 146/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 73.5295 - mse: 73.5295\n",
            "Epoch 147/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 73.1509 - mse: 73.1509\n",
            "Epoch 148/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 72.7778 - mse: 72.7778\n",
            "Epoch 149/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 72.4239 - mse: 72.4239\n",
            "Epoch 150/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 72.0437 - mse: 72.0437\n",
            "Epoch 151/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 71.6752 - mse: 71.6752\n",
            "Epoch 152/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 71.3282 - mse: 71.3282\n",
            "Epoch 153/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 70.9531 - mse: 70.9531\n",
            "Epoch 154/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 70.6037 - mse: 70.6037\n",
            "Epoch 155/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 70.2413 - mse: 70.2413\n",
            "Epoch 156/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 69.8887 - mse: 69.8887\n",
            "Epoch 157/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 69.5335 - mse: 69.5335\n",
            "Epoch 158/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 69.1976 - mse: 69.1976\n",
            "Epoch 159/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 68.8348 - mse: 68.8348\n",
            "Epoch 160/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 68.4976 - mse: 68.4976\n",
            "Epoch 161/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 68.1431 - mse: 68.1431\n",
            "Epoch 162/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 67.8093 - mse: 67.8093\n",
            "Epoch 163/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 67.4592 - mse: 67.4592\n",
            "Epoch 164/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 67.1226 - mse: 67.1226\n",
            "Epoch 165/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 66.7773 - mse: 66.7773\n",
            "Epoch 166/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 66.4517 - mse: 66.4517\n",
            "Epoch 167/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 66.1153 - mse: 66.1153\n",
            "Epoch 168/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 65.7766 - mse: 65.7766\n",
            "Epoch 169/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 65.4536 - mse: 65.4536\n",
            "Epoch 170/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 65.1191 - mse: 65.1191\n",
            "Epoch 171/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 64.8099 - mse: 64.8099\n",
            "Epoch 172/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 64.4606 - mse: 64.4606\n",
            "Epoch 173/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 64.1434 - mse: 64.1434\n",
            "Epoch 174/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 63.8255 - mse: 63.8255\n",
            "Epoch 175/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 63.5071 - mse: 63.5071\n",
            "Epoch 176/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 63.1843 - mse: 63.1843\n",
            "Epoch 177/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 62.8790 - mse: 62.8790\n",
            "Epoch 178/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 62.5505 - mse: 62.5505\n",
            "Epoch 179/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 62.2453 - mse: 62.2453\n",
            "Epoch 180/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 61.9315 - mse: 61.9315\n",
            "Epoch 181/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 61.6202 - mse: 61.6202\n",
            "Epoch 182/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 61.3240 - mse: 61.3240\n",
            "Epoch 183/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 61.0020 - mse: 61.0020\n",
            "Epoch 184/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 60.7112 - mse: 60.7112\n",
            "Epoch 185/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 60.4079 - mse: 60.4079\n",
            "Epoch 186/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 60.1018 - mse: 60.1018\n",
            "Epoch 187/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 59.8074 - mse: 59.8074\n",
            "Epoch 188/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 59.5068 - mse: 59.5068\n",
            "Epoch 189/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 59.2118 - mse: 59.2118\n",
            "Epoch 190/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 58.9195 - mse: 58.9195\n",
            "Epoch 191/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 58.6441 - mse: 58.6441\n",
            "Epoch 192/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 58.3420 - mse: 58.3420\n",
            "Epoch 193/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 58.0444 - mse: 58.0444\n",
            "Epoch 194/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 57.7656 - mse: 57.7656\n",
            "Epoch 195/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 57.4827 - mse: 57.4827\n",
            "Epoch 196/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 57.1985 - mse: 57.1985\n",
            "Epoch 197/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 56.9171 - mse: 56.9171\n",
            "Epoch 198/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 56.6486 - mse: 56.6486\n",
            "Epoch 199/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 56.3666 - mse: 56.3666\n",
            "Epoch 200/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 56.0888 - mse: 56.0888\n",
            "Epoch 201/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 55.8272 - mse: 55.8272\n",
            "Epoch 202/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 55.5370 - mse: 55.5370\n",
            "Epoch 203/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 55.2788 - mse: 55.2788\n",
            "Epoch 204/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 55.0062 - mse: 55.0062\n",
            "Epoch 205/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 54.7453 - mse: 54.7453\n",
            "Epoch 206/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 54.4815 - mse: 54.4815\n",
            "Epoch 207/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 54.2128 - mse: 54.2128\n",
            "Epoch 208/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 53.9507 - mse: 53.9507\n",
            "Epoch 209/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 53.6994 - mse: 53.6994\n",
            "Epoch 210/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 53.4378 - mse: 53.4378\n",
            "Epoch 211/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 53.1876 - mse: 53.1876\n",
            "Epoch 212/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 52.9343 - mse: 52.9343\n",
            "Epoch 213/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 52.6867 - mse: 52.6867\n",
            "Epoch 214/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 52.4312 - mse: 52.4312\n",
            "Epoch 215/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 52.1802 - mse: 52.1802\n",
            "Epoch 216/400\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 51.9375 - mse: 51.9375\n",
            "Epoch 217/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 51.7075 - mse: 51.7075\n",
            "Epoch 218/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 51.4566 - mse: 51.4566\n",
            "Epoch 219/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 51.2210 - mse: 51.2210\n",
            "Epoch 220/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 50.9741 - mse: 50.9741\n",
            "Epoch 221/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 50.7316 - mse: 50.7316\n",
            "Epoch 222/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 50.5103 - mse: 50.5103\n",
            "Epoch 223/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 50.2754 - mse: 50.2754\n",
            "Epoch 224/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 50.0355 - mse: 50.0355\n",
            "Epoch 225/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.8067 - mse: 49.8067\n",
            "Epoch 226/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.5794 - mse: 49.5794\n",
            "Epoch 227/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.3548 - mse: 49.3548\n",
            "Epoch 228/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.1376 - mse: 49.1376\n",
            "Epoch 229/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 48.9021 - mse: 48.9021\n",
            "Epoch 230/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 48.6941 - mse: 48.6941\n",
            "Epoch 231/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 48.4656 - mse: 48.4656\n",
            "Epoch 232/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 48.2462 - mse: 48.2462\n",
            "Epoch 233/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 48.0388 - mse: 48.0388\n",
            "Epoch 234/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 47.8226 - mse: 47.8226\n",
            "Epoch 235/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 47.6075 - mse: 47.6075\n",
            "Epoch 236/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 47.4014 - mse: 47.4014\n",
            "Epoch 237/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 47.1905 - mse: 47.1905\n",
            "Epoch 238/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 46.9968 - mse: 46.9968\n",
            "Epoch 239/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 46.7730 - mse: 46.7730\n",
            "Epoch 240/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 46.5750 - mse: 46.5750\n",
            "Epoch 241/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 46.3745 - mse: 46.3745\n",
            "Epoch 242/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 46.1754 - mse: 46.1754\n",
            "Epoch 243/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 45.9806 - mse: 45.9806\n",
            "Epoch 244/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 45.7799 - mse: 45.7799\n",
            "Epoch 245/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 45.5882 - mse: 45.5882\n",
            "Epoch 246/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 45.3892 - mse: 45.3892\n",
            "Epoch 247/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 45.2158 - mse: 45.2158\n",
            "Epoch 248/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 45.0093 - mse: 45.0093\n",
            "Epoch 249/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.8229 - mse: 44.8229\n",
            "Epoch 250/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 44.6359 - mse: 44.6359\n",
            "Epoch 251/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 44.4499 - mse: 44.4499\n",
            "Epoch 252/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 44.2682 - mse: 44.2682\n",
            "Epoch 253/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 44.0882 - mse: 44.0882\n",
            "Epoch 254/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 43.9177 - mse: 43.9177\n",
            "Epoch 255/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 43.7240 - mse: 43.7240\n",
            "Epoch 256/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 43.5523 - mse: 43.5523\n",
            "Epoch 257/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 43.3769 - mse: 43.3769\n",
            "Epoch 258/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 43.1986 - mse: 43.1986\n",
            "Epoch 259/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 43.0357 - mse: 43.0357\n",
            "Epoch 260/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 42.8590 - mse: 42.8590\n",
            "Epoch 261/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 42.6862 - mse: 42.6862\n",
            "Epoch 262/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 42.5257 - mse: 42.5257\n",
            "Epoch 263/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 42.3636 - mse: 42.3636\n",
            "Epoch 264/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.1940 - mse: 42.1940\n",
            "Epoch 265/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.0349 - mse: 42.0349\n",
            "Epoch 266/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 41.8775 - mse: 41.8775\n",
            "Epoch 267/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 41.7134 - mse: 41.7134\n",
            "Epoch 268/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.5577 - mse: 41.5577\n",
            "Epoch 269/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.3931 - mse: 41.3931\n",
            "Epoch 270/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 41.2482 - mse: 41.2482\n",
            "Epoch 271/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.0863 - mse: 41.0863\n",
            "Epoch 272/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.9392 - mse: 40.9392\n",
            "Epoch 273/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.7891 - mse: 40.7891\n",
            "Epoch 274/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 40.6462 - mse: 40.6462\n",
            "Epoch 275/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 40.4977 - mse: 40.4977\n",
            "Epoch 276/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.3475 - mse: 40.3475\n",
            "Epoch 277/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.2016 - mse: 40.2016\n",
            "Epoch 278/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.0646 - mse: 40.0646\n",
            "Epoch 279/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.9255 - mse: 39.9255\n",
            "Epoch 280/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 39.7829 - mse: 39.7829\n",
            "Epoch 281/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.6370 - mse: 39.6370\n",
            "Epoch 282/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.5055 - mse: 39.5055\n",
            "Epoch 283/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 39.3707 - mse: 39.3707\n",
            "Epoch 284/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 39.2341 - mse: 39.2341\n",
            "Epoch 285/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.1019 - mse: 39.1019\n",
            "Epoch 286/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.9760 - mse: 38.9760\n",
            "Epoch 287/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.8400 - mse: 38.8400\n",
            "Epoch 288/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 38.7176 - mse: 38.7176\n",
            "Epoch 289/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 38.5904 - mse: 38.5904\n",
            "Epoch 290/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 38.4644 - mse: 38.4644\n",
            "Epoch 291/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 38.3405 - mse: 38.3405\n",
            "Epoch 292/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.2140 - mse: 38.2140\n",
            "Epoch 293/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.0941 - mse: 38.0941\n",
            "Epoch 294/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.9757 - mse: 37.9757\n",
            "Epoch 295/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 37.8639 - mse: 37.8639\n",
            "Epoch 296/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.7362 - mse: 37.7362\n",
            "Epoch 297/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.6283 - mse: 37.6283\n",
            "Epoch 298/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.5102 - mse: 37.5102\n",
            "Epoch 299/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.4002 - mse: 37.4002\n",
            "Epoch 300/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.2821 - mse: 37.2821\n",
            "Epoch 301/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.1705 - mse: 37.1705\n",
            "Epoch 302/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.0616 - mse: 37.0616\n",
            "Epoch 303/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.9560 - mse: 36.9560\n",
            "Epoch 304/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 36.8492 - mse: 36.8492\n",
            "Epoch 305/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 36.7413 - mse: 36.7413\n",
            "Epoch 306/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.6380 - mse: 36.6380\n",
            "Epoch 307/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 36.5367 - mse: 36.5367\n",
            "Epoch 308/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 36.4370 - mse: 36.4370\n",
            "Epoch 309/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 36.3387 - mse: 36.3387\n",
            "Epoch 310/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.2475 - mse: 36.2475\n",
            "Epoch 311/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.1352 - mse: 36.1352\n",
            "Epoch 312/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.0424 - mse: 36.0424\n",
            "Epoch 313/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 35.9459 - mse: 35.9459\n",
            "Epoch 314/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 35.8555 - mse: 35.8555\n",
            "Epoch 315/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 35.7604 - mse: 35.7604\n",
            "Epoch 316/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.6766 - mse: 35.6766\n",
            "Epoch 317/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 35.5905 - mse: 35.5905\n",
            "Epoch 318/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 35.4901 - mse: 35.4901\n",
            "Epoch 319/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.4015 - mse: 35.4015\n",
            "Epoch 320/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.3156 - mse: 35.3156\n",
            "Epoch 321/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 35.2442 - mse: 35.2442\n",
            "Epoch 322/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 35.1404 - mse: 35.1404\n",
            "Epoch 323/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.0637 - mse: 35.0637\n",
            "Epoch 324/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.9822 - mse: 34.9822\n",
            "Epoch 325/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.8958 - mse: 34.8958\n",
            "Epoch 326/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.8180 - mse: 34.8180\n",
            "Epoch 327/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.7415 - mse: 34.7415\n",
            "Epoch 328/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 34.6635 - mse: 34.6635\n",
            "Epoch 329/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 34.5848 - mse: 34.5848\n",
            "Epoch 330/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 34.5116 - mse: 34.5116\n",
            "Epoch 331/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.4383 - mse: 34.4383\n",
            "Epoch 332/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 34.3661 - mse: 34.3661\n",
            "Epoch 333/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 34.2925 - mse: 34.2925\n",
            "Epoch 334/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 34.2275 - mse: 34.2275\n",
            "Epoch 335/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 34.1505 - mse: 34.1505\n",
            "Epoch 336/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.0904 - mse: 34.0904\n",
            "Epoch 337/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 34.0210 - mse: 34.0210\n",
            "Epoch 338/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 33.9475 - mse: 33.9475\n",
            "Epoch 339/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 33.8773 - mse: 33.8773\n",
            "Epoch 340/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 33.8105 - mse: 33.8105\n",
            "Epoch 341/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.7537 - mse: 33.7537\n",
            "Epoch 342/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 33.6834 - mse: 33.6834\n",
            "Epoch 343/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.6213 - mse: 33.6213\n",
            "Epoch 344/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 33.5576 - mse: 33.5576\n",
            "Epoch 345/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 33.4981 - mse: 33.4981\n",
            "Epoch 346/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 33.4410 - mse: 33.4410\n",
            "Epoch 347/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 33.3818 - mse: 33.3818\n",
            "Epoch 348/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.3248 - mse: 33.3248\n",
            "Epoch 349/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.2668 - mse: 33.2668\n",
            "Epoch 350/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.2109 - mse: 33.2109\n",
            "Epoch 351/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.1582 - mse: 33.1582\n",
            "Epoch 352/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.1095 - mse: 33.1095\n",
            "Epoch 353/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 33.0530 - mse: 33.0530\n",
            "Epoch 354/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.9959 - mse: 32.9959\n",
            "Epoch 355/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.9483 - mse: 32.9483\n",
            "Epoch 356/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.9090 - mse: 32.9090\n",
            "Epoch 357/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.8418 - mse: 32.8418\n",
            "Epoch 358/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.8015 - mse: 32.8015\n",
            "Epoch 359/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 32.7509 - mse: 32.7509\n",
            "Epoch 360/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 32.7049 - mse: 32.7049\n",
            "Epoch 361/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 32.6631 - mse: 32.6631\n",
            "Epoch 362/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.6133 - mse: 32.6133\n",
            "Epoch 363/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 32.5669 - mse: 32.5669\n",
            "Epoch 364/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.5219 - mse: 32.5219\n",
            "Epoch 365/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.4867 - mse: 32.4867\n",
            "Epoch 366/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.4384 - mse: 32.4384\n",
            "Epoch 367/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 32.3969 - mse: 32.3969\n",
            "Epoch 368/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 32.3617 - mse: 32.3617\n",
            "Epoch 369/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.3151 - mse: 32.3151\n",
            "Epoch 370/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.2771 - mse: 32.2771\n",
            "Epoch 371/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 32.2377 - mse: 32.2377\n",
            "Epoch 372/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.2013 - mse: 32.2013\n",
            "Epoch 373/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.1616 - mse: 32.1616\n",
            "Epoch 374/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.1255 - mse: 32.1255\n",
            "Epoch 375/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 32.0899 - mse: 32.0899\n",
            "Epoch 376/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.0631 - mse: 32.0631\n",
            "Epoch 377/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 32.0283 - mse: 32.0283\n",
            "Epoch 378/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.9845 - mse: 31.9845\n",
            "Epoch 379/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.9542 - mse: 31.9542\n",
            "Epoch 380/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.9165 - mse: 31.9165\n",
            "Epoch 381/400\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 31.8851 - mse: 31.8851\n",
            "Epoch 382/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 31.8590 - mse: 31.8590\n",
            "Epoch 383/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.8202 - mse: 31.8202\n",
            "Epoch 384/400\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 31.7958 - mse: 31.7958\n",
            "Epoch 385/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 31.7667 - mse: 31.7667\n",
            "Epoch 386/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 31.7346 - mse: 31.7346\n",
            "Epoch 387/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.7097 - mse: 31.7097\n",
            "Epoch 388/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.6792 - mse: 31.6792\n",
            "Epoch 389/400\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 31.6510 - mse: 31.6510\n",
            "Epoch 390/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.6277 - mse: 31.6277\n",
            "Epoch 391/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.6020 - mse: 31.6020\n",
            "Epoch 392/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.5737 - mse: 31.5737\n",
            "Epoch 393/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.5441 - mse: 31.5441\n",
            "Epoch 394/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.5225 - mse: 31.5225\n",
            "Epoch 395/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.5002 - mse: 31.5002\n",
            "Epoch 396/400\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 31.4812 - mse: 31.4812\n",
            "Epoch 397/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.4546 - mse: 31.4546\n",
            "Epoch 398/400\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.4329 - mse: 31.4329\n",
            "Epoch 399/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.4062 - mse: 31.4062\n",
            "Epoch 400/400\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 31.3888 - mse: 31.3888\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0e7f75dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras로 학습된 모델을 이용하여 주택 가격 예측 수행. "
      ],
      "metadata": {
        "id": "jaz9vcE3iju1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(scaled_features)\n",
        "bostonDF['KERAS_PREDICTED_PRICE'] = predicted\n",
        "bostonDF.head(10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T02:38:26.924528Z",
          "iopub.execute_input": "2022-02-10T02:38:26.925044Z",
          "iopub.status.idle": "2022-02-10T02:38:27.060719Z",
          "shell.execute_reply.started": "2022-02-10T02:38:26.925012Z",
          "shell.execute_reply": "2022-02-10T02:38:27.059899Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "2naOvuIbiju1",
        "outputId": "23b26fc7-f943-44a9-a0e3-f5639026e14d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bb12abc9-9358-45a4-8f3c-b164d68ae6be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE</th>\n",
              "      <th>KERAS_PREDICTED_PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.298946</td>\n",
              "      <td>28.106625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>24.270017</td>\n",
              "      <td>25.095121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>28.842337</td>\n",
              "      <td>31.375834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>28.551276</td>\n",
              "      <td>31.143904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>28.244935</td>\n",
              "      <td>30.480211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>25.692891</td>\n",
              "      <td>27.327185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.626138</td>\n",
              "      <td>21.423622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>19.827786</td>\n",
              "      <td>18.394613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>14.056539</td>\n",
              "      <td>9.953367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>19.945095</td>\n",
              "      <td>18.783373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb12abc9-9358-45a4-8f3c-b164d68ae6be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb12abc9-9358-45a4-8f3c-b164d68ae6be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb12abc9-9358-45a4-8f3c-b164d68ae6be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  ...  PRICE  PREDICTED_PRICE  KERAS_PREDICTED_PRICE\n",
              "0  0.00632  18.0   2.31  ...   24.0        26.298946              28.106625\n",
              "1  0.02731   0.0   7.07  ...   21.6        24.270017              25.095121\n",
              "2  0.02729   0.0   7.07  ...   34.7        28.842337              31.375834\n",
              "3  0.03237   0.0   2.18  ...   33.4        28.551276              31.143904\n",
              "4  0.06905   0.0   2.18  ...   36.2        28.244935              30.480211\n",
              "5  0.02985   0.0   2.18  ...   28.7        25.692891              27.327185\n",
              "6  0.08829  12.5   7.87  ...   22.9        21.626138              21.423622\n",
              "7  0.14455  12.5   7.87  ...   27.1        19.827786              18.394613\n",
              "8  0.21124  12.5   7.87  ...   16.5        14.056539               9.953367\n",
              "9  0.17004  12.5   7.87  ...   18.9        19.945095              18.783373\n",
              "\n",
              "[10 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n",
        "* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n",
        "* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용"
      ],
      "metadata": {
        "id": "JeWlTYkYiju1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "bostonDF['PRICE'] = boston.target\n",
        "print(bostonDF.shape)\n",
        "bostonDF.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:22.154635Z",
          "iopub.execute_input": "2022-02-10T06:10:22.155396Z",
          "iopub.status.idle": "2022-02-10T06:10:22.20071Z",
          "shell.execute_reply.started": "2022-02-10T06:10:22.155353Z",
          "shell.execute_reply": "2022-02-10T06:10:22.199519Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "A-KCQHgMiju2",
        "outputId": "c0ad86c2-46c9-4d65-bc1c-375d20eeec40"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d48e9314-a653-4719-8378-d72810faa74b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d48e9314-a653-4719-8378-d72810faa74b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d48e9314-a653-4719-8378-d72810faa74b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d48e9314-a653-4719-8378-d72810faa74b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  PRICE\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98   24.0\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14   21.6\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03   34.7\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94   33.4\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33   36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD 기반으로 Weight/Bias update 값 구하기"
      ],
      "metadata": {
        "id": "JaCxVIKyiju2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rm_sgd는 이제 batch size만큼 들어올거임. 지금은 그냥 SGD니까 1개만 들어옴. 아래 변수 다 한개임\n",
        "def get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n",
        "    \n",
        "    # 데이터 건수\n",
        "    N = target_sgd.shape[0]\n",
        "    # 예측 값. \n",
        "    predicted_sgd = w1 * rm_sgd + w2*lstat_sgd + bias\n",
        "    # 실제값과 예측값의 차이 \n",
        "    diff_sgd = target_sgd - predicted_sgd\n",
        "    # bias 를 array 기반으로 구하기 위해서 설정. \n",
        "    bias_factors = np.ones((N,))\n",
        "    \n",
        "    # weight와 bias를 얼마나 update할 것인지를 계산.  \n",
        "    w1_update = -(2/N)*learning_rate*(np.dot(rm_sgd.T, diff_sgd))\n",
        "    w2_update = -(2/N)*learning_rate*(np.dot(lstat_sgd.T, diff_sgd))\n",
        "    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_sgd))\n",
        "    \n",
        "    # MSE를 계산했던 부분은 이제 밖에서 계산할거임 mse_loss = np.mean(np.square(diff))\n",
        "    \n",
        "    # weight와 bias가 update되어야 할 값 반환 \n",
        "    return bias_update, w1_update, w2_update"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:22.546745Z",
          "iopub.execute_input": "2022-02-10T06:10:22.547907Z",
          "iopub.status.idle": "2022-02-10T06:10:22.556291Z",
          "shell.execute_reply.started": "2022-02-10T06:10:22.547839Z",
          "shell.execute_reply": "2022-02-10T06:10:22.555255Z"
        },
        "trusted": true,
        "id": "OzlqYG4Kiju2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD 수행하기"
      ],
      "metadata": {
        "id": "CaV-xIZliju2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bostonDF['PRICE'].values.shape)\n",
        "print(np.random.choice(bostonDF['PRICE'].values.shape[0], 1))\n",
        "print(np.random.choice(506, 1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:22.963141Z",
          "iopub.execute_input": "2022-02-10T06:10:22.963617Z",
          "iopub.status.idle": "2022-02-10T06:10:22.971183Z",
          "shell.execute_reply.started": "2022-02-10T06:10:22.963585Z",
          "shell.execute_reply": "2022-02-10T06:10:22.970483Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US12wVlwiju3",
        "outputId": "8cc16c27-6f8c-4b50-b471-22ac1fb72541"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506,)\n",
            "[427]\n",
            "[93]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \n",
        "def st_gradient_descent(features, target, iter_epochs=1000, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n",
        "    np.random.seed = 2021\n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.zeros((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "    print()\n",
        "    \n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0]\n",
        "    lstat = features[:, 1]\n",
        "    \n",
        "    \n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
        "    for i in range(iter_epochs):\n",
        "        # iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. 추출할 데이터의 인덱스를 random.choice() 로 선택. \n",
        "        stochastic_index = np.random.choice(target.shape[0], 1) # target의 shape은 (갯수, 1)\n",
        "        rm_sgd = rm[stochastic_index]\n",
        "        lstat_sgd = lstat[stochastic_index]\n",
        "        target_sgd = target[stochastic_index]\n",
        "        # SGD 기반으로 Weight/Bias의 Update를 구함.  \n",
        "        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate)\n",
        "        \n",
        "        # SGD로 구한 weight/bias의 update 적용. \n",
        "        w1 = w1 - w1_update\n",
        "        w2 = w2 - w2_update\n",
        "        bias = bias - bias_update\n",
        "        if (verbose):\n",
        "            if (i == 0) or ((i+1)%200==0):\n",
        "                print('Epoch:', i+1,'/', iter_epochs)\n",
        "                # Loss는 전체 학습 데이터 기반으로 구해야 함. ###############중요#########\n",
        "                predicted = w1 * rm + w2*lstat + bias # 전체 갯수\n",
        "                diff = target - predicted # 전체 갯수\n",
        "                mse_loss = np.mean(np.square(diff))\n",
        "                print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n",
        "        \n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:23.152664Z",
          "iopub.execute_input": "2022-02-10T06:10:23.153652Z",
          "iopub.status.idle": "2022-02-10T06:10:23.168169Z",
          "shell.execute_reply.started": "2022-02-10T06:10:23.153612Z",
          "shell.execute_reply": "2022-02-10T06:10:23.167197Z"
        },
        "trusted": true,
        "id": "rQ7P2u0Liju3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n",
        "\n",
        "w1, w2, bias = st_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:23.387722Z",
          "iopub.execute_input": "2022-02-10T06:10:23.388841Z",
          "iopub.status.idle": "2022-02-10T06:10:23.478896Z",
          "shell.execute_reply.started": "2022-02-10T06:10:23.388786Z",
          "shell.execute_reply": "2022-02-10T06:10:23.477998Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dxlCxaRiju3",
        "outputId": "1fdfb59e-15f5-4179-f3d0-eed0e49111c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최초 w1, w2, bias: [0.] [0.] [0.]\n",
            "\n",
            "Epoch: 1 / 1000\n",
            "w1: [0.98946158] w2: [0.08002208] bias: [1.] loss: 523.6254285201774\n",
            "Epoch: 200 / 1000\n",
            "w1: [11.15345637] w2: [-0.43068693] bias: [15.63170774] loss: 67.98076314216202\n",
            "Epoch: 400 / 1000\n",
            "w1: [14.26621253] w2: [-4.69173892] bias: [16.80524556] loss: 54.184773352279514\n",
            "Epoch: 600 / 1000\n",
            "w1: [15.70529429] w2: [-8.52057717] bias: [15.76766723] loss: 47.61031347380366\n",
            "Epoch: 800 / 1000\n",
            "w1: [17.59735851] w2: [-11.41806446] bias: [16.54007279] loss: 40.95632134715162\n",
            "Epoch: 1000 / 1000\n",
            "w1: [18.59072594] w2: [-13.0437515] bias: [16.87342663] loss: 38.40285510413929\n",
            "##### 최종 w1, w2, bias #######\n",
            "[18.59072594] [-13.0437515] [16.87342663]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "bostonDF['PREDICTED_PRICE_SGD'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:23.759223Z",
          "iopub.execute_input": "2022-02-10T06:10:23.760392Z",
          "iopub.status.idle": "2022-02-10T06:10:23.79426Z",
          "shell.execute_reply.started": "2022-02-10T06:10:23.760336Z",
          "shell.execute_reply": "2022-02-10T06:10:23.79293Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "vDQ9zxaGiju3",
        "outputId": "cc1390e7-032b-479e-eb43-a87cda0ba7f9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-975f5a3c-1db6-46a6-ab21-f16a962af0ac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE_SGD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.439906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>24.394043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>28.954733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>28.680936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>28.351467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>25.840615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.752975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>19.904206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>14.097085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>20.043619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-975f5a3c-1db6-46a6-ab21-f16a962af0ac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-975f5a3c-1db6-46a6-ab21-f16a962af0ac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-975f5a3c-1db6-46a6-ab21-f16a962af0ac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS  ...       B  LSTAT  PRICE  PREDICTED_PRICE_SGD\n",
              "0  0.00632  18.0   2.31   0.0  ...  396.90   4.98   24.0            26.439906\n",
              "1  0.02731   0.0   7.07   0.0  ...  396.90   9.14   21.6            24.394043\n",
              "2  0.02729   0.0   7.07   0.0  ...  392.83   4.03   34.7            28.954733\n",
              "3  0.03237   0.0   2.18   0.0  ...  394.63   2.94   33.4            28.680936\n",
              "4  0.06905   0.0   2.18   0.0  ...  396.90   5.33   36.2            28.351467\n",
              "5  0.02985   0.0   2.18   0.0  ...  394.12   5.21   28.7            25.840615\n",
              "6  0.08829  12.5   7.87   0.0  ...  395.60  12.43   22.9            21.752975\n",
              "7  0.14455  12.5   7.87   0.0  ...  396.90  19.15   27.1            19.904206\n",
              "8  0.21124  12.5   7.87   0.0  ...  386.63  29.93   16.5            14.097085\n",
              "9  0.17004  12.5   7.87   0.0  ...  386.71  17.10   18.9            20.043619\n",
              "\n",
              "[10 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행"
      ],
      "metadata": {
        "id": "UTqIX_5Liju4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이번에는 batch size만큼 들어옴\n",
        "def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n",
        "    \n",
        "    # 데이터 건수\n",
        "    N = target_batch.shape[0]\n",
        "    # 예측 값. \n",
        "    predicted_batch = w1 * rm_batch+ w2 * lstat_batch + bias\n",
        "    # 실제값과 예측값의 차이 \n",
        "    diff_batch = target_batch - predicted_batch\n",
        "    # bias 를 array 기반으로 구하기 위해서 설정. \n",
        "    bias_factors = np.ones((N,))\n",
        "    \n",
        "    # weight와 bias를 얼마나 update할 것인지를 계산.  \n",
        "    w1_update = -(2/N)*learning_rate*(np.dot(rm_batch.T, diff_batch))\n",
        "    w2_update = -(2/N)*learning_rate*(np.dot(lstat_batch.T, diff_batch))\n",
        "    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_batch))\n",
        "    \n",
        "    # Mean Squared Error값을 계산. \n",
        "    #mse_loss = np.mean(np.square(diff))\n",
        "    \n",
        "    # weight와 bias가 update되어야 할 값 반환 \n",
        "    return bias_update, w1_update, w2_update"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:24.820512Z",
          "iopub.execute_input": "2022-02-10T06:10:24.820875Z",
          "iopub.status.idle": "2022-02-10T06:10:24.829325Z",
          "shell.execute_reply.started": "2022-02-10T06:10:24.820842Z",
          "shell.execute_reply": "2022-02-10T06:10:24.828343Z"
        },
        "trusted": true,
        "id": "zFelSsCIiju4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \n",
        "def batch_random_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n",
        "    np.random.seed = 2021\n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.zeros((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "    \n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0]\n",
        "    lstat = features[:, 1]\n",
        "    \n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
        "    for i in range(iter_epochs):\n",
        "        # batch_size 갯수만큼 데이터를 임의로 선택. \n",
        "        batch_indexes = np.random.choice(target.shape[0], batch_size)\n",
        "        rm_batch = rm[batch_indexes]\n",
        "        lstat_batch = lstat[batch_indexes]\n",
        "        target_batch = target[batch_indexes]\n",
        "        # Batch GD 기반으로 Weight/Bias의 Update를 구함. \n",
        "        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n",
        "        \n",
        "        # Batch GD로 구한 weight/bias의 update 적용. \n",
        "        w1 = w1 - w1_update\n",
        "        w2 = w2 - w2_update\n",
        "        bias = bias - bias_update\n",
        "        if verbose:\n",
        "            if (i == 0) or ((i+1)%200==0):\n",
        "                print('Epoch:', i+1,'/', iter_epochs)\n",
        "                # Loss는 전체 학습 데이터 기반으로 구해야 함.\n",
        "                predicted = w1 * rm + w2*lstat + bias\n",
        "                diff = target - predicted\n",
        "                mse_loss = np.mean(np.square(diff))\n",
        "                print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n",
        "        \n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:25.560138Z",
          "iopub.execute_input": "2022-02-10T06:10:25.560421Z",
          "iopub.status.idle": "2022-02-10T06:10:25.574001Z",
          "shell.execute_reply.started": "2022-02-10T06:10:25.560392Z",
          "shell.execute_reply": "2022-02-10T06:10:25.572818Z"
        },
        "trusted": true,
        "id": "wmWOKoLDiju4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, bias = batch_random_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, batch_size=30, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:26.329217Z",
          "iopub.execute_input": "2022-02-10T06:10:26.329541Z",
          "iopub.status.idle": "2022-02-10T06:10:26.417034Z",
          "shell.execute_reply.started": "2022-02-10T06:10:26.329509Z",
          "shell.execute_reply": "2022-02-10T06:10:26.415993Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pCJV165iju4",
        "outputId": "8361455a-3bd2-4dea-84a8-54b1cd7a7919"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최초 w1, w2, bias: [0.] [0.] [0.]\n",
            "Epoch: 1 / 1000\n",
            "w1: [0.26838742] w2: [0.09756275] bias: [0.4582] loss: 564.054502216033\n",
            "Epoch: 200 / 1000\n",
            "w1: [11.2802044] w2: [0.03634203] bias: [16.29739972] loss: 67.53328185126713\n",
            "Epoch: 400 / 1000\n",
            "w1: [13.8687651] w2: [-4.10075609] bias: [16.56593278] loss: 55.62332721077435\n",
            "Epoch: 600 / 1000\n",
            "w1: [15.80641394] w2: [-7.52405831] bias: [16.60538157] loss: 47.759391848423995\n",
            "Epoch: 800 / 1000\n",
            "w1: [17.40553661] w2: [-10.38426187] bias: [16.57877093] loss: 42.33082469901731\n",
            "Epoch: 1000 / 1000\n",
            "w1: [18.6875145] w2: [-12.79261875] bias: [16.40344286] loss: 38.650815537849766\n",
            "##### 최종 w1, w2, bias #######\n",
            "[18.6875145] [-12.79261875] [16.40344286]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "bostonDF['PREDICTED_PRICE_BATCH_RANDOM'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:27.151358Z",
          "iopub.execute_input": "2022-02-10T06:10:27.151712Z",
          "iopub.status.idle": "2022-02-10T06:10:27.18546Z",
          "shell.execute_reply.started": "2022-02-10T06:10:27.151675Z",
          "shell.execute_reply": "2022-02-10T06:10:27.184543Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "fyFlZJKbiju4",
        "outputId": "878b61fd-5e40-4794-912b-9e0342dee364"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8f20ed90-4306-4469-958f-c3b7a9ca296e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE_SGD</th>\n",
              "      <th>PREDICTED_PRICE_BATCH_RANDOM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.439906</td>\n",
              "      <td>26.048340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>24.394043</td>\n",
              "      <td>24.028449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>28.954733</td>\n",
              "      <td>28.567896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>28.680936</td>\n",
              "      <td>28.283078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>28.351467</td>\n",
              "      <td>27.972934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>25.840615</td>\n",
              "      <td>25.447954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.752975</td>\n",
              "      <td>21.402594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>19.904206</td>\n",
              "      <td>19.603360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>14.097085</td>\n",
              "      <td>13.860908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>20.043619</td>\n",
              "      <td>19.725452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f20ed90-4306-4469-958f-c3b7a9ca296e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f20ed90-4306-4469-958f-c3b7a9ca296e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f20ed90-4306-4469-958f-c3b7a9ca296e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  ...  PREDICTED_PRICE_SGD  PREDICTED_PRICE_BATCH_RANDOM\n",
              "0  0.00632  18.0  ...            26.439906                     26.048340\n",
              "1  0.02731   0.0  ...            24.394043                     24.028449\n",
              "2  0.02729   0.0  ...            28.954733                     28.567896\n",
              "3  0.03237   0.0  ...            28.680936                     28.283078\n",
              "4  0.06905   0.0  ...            28.351467                     27.972934\n",
              "5  0.02985   0.0  ...            25.840615                     25.447954\n",
              "6  0.08829  12.5  ...            21.752975                     21.402594\n",
              "7  0.14455  12.5  ...            19.904206                     19.603360\n",
              "8  0.21124  12.5  ...            14.097085                     13.860908\n",
              "9  0.17004  12.5  ...            20.043619                     19.725452\n",
              "\n",
              "[10 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행"
      ],
      "metadata": {
        "id": "OSYoQeZjiju5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = [1,2,3,4]\n",
        "b[1:9]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:10:28.706597Z",
          "iopub.execute_input": "2022-02-10T06:10:28.706949Z",
          "iopub.status.idle": "2022-02-10T06:10:28.714452Z",
          "shell.execute_reply.started": "2022-02-10T06:10:28.706913Z",
          "shell.execute_reply": "2022-02-10T06:10:28.713532Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct1rBU4miju5",
        "outputId": "2535f578-4688-4b93-c5a1-cd414affa233"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \n",
        "def batch_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n",
        "    np.random.seed = 2021\n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.ones((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "    print()\n",
        "    \n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0]\n",
        "    lstat = features[:, 1]\n",
        "    \n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
        "    for i in range(iter_epochs):\n",
        "        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n",
        "        for batch_step in range(0, target.shape[0], batch_size):\n",
        "            # batch_size만큼 순차적인 데이터를 가져옴. \n",
        "            rm_batch = rm[batch_step:batch_step + batch_size]\n",
        "            lstat_batch = lstat[batch_step:batch_step + batch_size]\n",
        "            target_batch = target[batch_step:batch_step + batch_size]\n",
        "        \n",
        "            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n",
        "\n",
        "            # Batch GD로 구한 weight/bias의 update 적용. \n",
        "            w1 = w1 - w1_update\n",
        "            w2 = w2 - w2_update\n",
        "            bias = bias - bias_update\n",
        "        \n",
        "            if verbose:\n",
        "                if (i == 0) or ((i+1)%200==0):\n",
        "                    print('Epoch:', i+1,'/', iter_epochs, 'batch step:', batch_step)\n",
        "                    # Loss는 전체 학습 데이터 기반으로 구해야 함.\n",
        "                    predicted = w1 * rm + w2*lstat + bias\n",
        "                    diff = target - predicted\n",
        "                    mse_loss = np.mean(np.square(diff))\n",
        "                    print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n",
        "                    print()\n",
        "        \n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:15:36.24809Z",
          "iopub.execute_input": "2022-02-10T06:15:36.248417Z",
          "iopub.status.idle": "2022-02-10T06:15:36.262336Z",
          "shell.execute_reply.started": "2022-02-10T06:15:36.248386Z",
          "shell.execute_reply": "2022-02-10T06:15:36.261387Z"
        },
        "trusted": true,
        "id": "7HLok9i1iju5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch size를 키우면 빠르지만 컴퓨터 성능이 좋아야 함"
      ],
      "metadata": {
        "id": "qzkJ81Uiiju5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, bias = batch_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, batch_size=100, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:15:37.969218Z",
          "iopub.execute_input": "2022-02-10T06:15:37.970208Z",
          "iopub.status.idle": "2022-02-10T06:15:38.247399Z",
          "shell.execute_reply.started": "2022-02-10T06:15:37.970144Z",
          "shell.execute_reply": "2022-02-10T06:15:38.246294Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi2AQwcjiju6",
        "outputId": "ce54bb30-8639-4bbd-cedd-2eac83f42c11"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최초 w1, w2, bias: [0.] [0.] [1.]\n",
            "\n",
            "Epoch: 1 / 1000 batch step: 0\n",
            "w1: [0.22765327] w2: [0.09261079] bias: [1.42618] loss: 523.5937495327876\n",
            "\n",
            "Epoch: 1 / 1000 batch step: 100\n",
            "w1: [0.48395592] w2: [0.20281825] bias: [1.87898758] loss: 497.8524182922854\n",
            "\n",
            "Epoch: 1 / 1000 batch step: 200\n",
            "w1: [0.84842665] w2: [0.29183783] bias: [2.43335843] loss: 466.68322395774254\n",
            "\n",
            "Epoch: 1 / 1000 batch step: 300\n",
            "w1: [1.03012045] w2: [0.37472713] bias: [2.79237108] loss: 448.0855850315973\n",
            "\n",
            "Epoch: 1 / 1000 batch step: 400\n",
            "w1: [1.14785609] w2: [0.47482372] bias: [3.03319319] loss: 435.5713366632866\n",
            "\n",
            "Epoch: 1 / 1000 batch step: 500\n",
            "w1: [1.32640005] w2: [0.53479778] bias: [3.35007373] loss: 419.5514184326998\n",
            "\n",
            "Epoch: 200 / 1000 batch step: 0\n",
            "w1: [18.47275292] w2: [-12.60110566] bias: [15.86951596] loss: 39.613743745462855\n",
            "\n",
            "Epoch: 200 / 1000 batch step: 100\n",
            "w1: [18.50990386] w2: [-12.59682541] bias: [15.92148701] loss: 39.48061817856162\n",
            "\n",
            "Epoch: 200 / 1000 batch step: 200\n",
            "w1: [18.58485658] w2: [-12.58289045] bias: [16.0224759] loss: 39.252794816572894\n",
            "\n",
            "Epoch: 200 / 1000 batch step: 300\n",
            "w1: [18.57884817] w2: [-12.59360292] bias: [16.01751922] loss: 39.25876459472282\n",
            "\n",
            "Epoch: 200 / 1000 batch step: 400\n",
            "w1: [18.54089617] w2: [-12.63017383] bias: [15.94507495] loss: 39.38093968203991\n",
            "\n",
            "Epoch: 200 / 1000 batch step: 500\n",
            "w1: [18.49853246] w2: [-12.64534458] bias: [15.86440556] loss: 39.5572897132992\n",
            "\n",
            "Epoch: 400 / 1000 batch step: 0\n",
            "w1: [22.55833131] w2: [-18.44596748] bias: [15.38385456] loss: 33.221446989166886\n",
            "\n",
            "Epoch: 400 / 1000 batch step: 100\n",
            "w1: [22.59338199] w2: [-18.43491011] bias: [15.43878793] loss: 33.07894215311436\n",
            "\n",
            "Epoch: 400 / 1000 batch step: 200\n",
            "w1: [22.65353363] w2: [-18.42095037] bias: [15.52140954] loss: 32.87858743177965\n",
            "\n",
            "Epoch: 400 / 1000 batch step: 300\n",
            "w1: [22.6470755] w2: [-18.42256526] bias: [15.52309281] loss: 32.88294091958596\n",
            "\n",
            "Epoch: 400 / 1000 batch step: 400\n",
            "w1: [22.61948664] w2: [-18.44446266] bias: [15.47534203] loss: 32.987169434450216\n",
            "\n",
            "Epoch: 400 / 1000 batch step: 500\n",
            "w1: [22.56937829] w2: [-18.46107246] bias: [15.3821654] loss: 33.2113729406943\n",
            "\n",
            "Epoch: 600 / 1000 batch step: 0\n",
            "w1: [24.36226344] w2: [-20.32145182] bias: [14.96740145] loss: 32.10898957214329\n",
            "\n",
            "Epoch: 600 / 1000 batch step: 100\n",
            "w1: [24.39652269] w2: [-20.30800054] bias: [15.02337938] loss: 31.96090590688257\n",
            "\n",
            "Epoch: 600 / 1000 batch step: 200\n",
            "w1: [24.45119461] w2: [-20.29404565] bias: [15.09928227] loss: 31.772076068212836\n",
            "\n",
            "Epoch: 600 / 1000 batch step: 300\n",
            "w1: [24.44461148] w2: [-20.29243319] bias: [15.10356393] loss: 31.771607374949518\n",
            "\n",
            "Epoch: 600 / 1000 batch step: 400\n",
            "w1: [24.42047868] w2: [-20.30934283] bias: [15.0642179] loss: 31.863958957478257\n",
            "\n",
            "Epoch: 600 / 1000 batch step: 500\n",
            "w1: [24.36772575] w2: [-20.32641479] bias: [14.96683321] loss: 32.10499427424435\n",
            "\n",
            "Epoch: 800 / 1000 batch step: 0\n",
            "w1: [25.31009491] w2: [-20.81736253] bias: [14.60836384] loss: 31.860252414784345\n",
            "\n",
            "Epoch: 800 / 1000 batch step: 100\n",
            "w1: [25.34403266] w2: [-20.80306882] bias: [14.66470777] loss: 31.71021485190564\n",
            "\n",
            "Epoch: 800 / 1000 batch step: 200\n",
            "w1: [25.39655803] w2: [-20.78912743] bias: [14.73804607] loss: 31.52646330557838\n",
            "\n",
            "Epoch: 800 / 1000 batch step: 300\n",
            "w1: [25.38996028] w2: [-20.7863673] bias: [14.74346157] loss: 31.523404531749232\n",
            "\n",
            "Epoch: 800 / 1000 batch step: 400\n",
            "w1: [25.3668662] w2: [-20.80169126] bias: [14.70679747] loss: 31.611367448716567\n",
            "\n",
            "Epoch: 800 / 1000 batch step: 500\n",
            "w1: [25.31326167] w2: [-20.8188858] bias: [14.60811407] loss: 31.857630369939795\n",
            "\n",
            "Epoch: 1000 / 1000 batch step: 0\n",
            "w1: [25.91314986] w2: [-20.84735186] bias: [14.29813604] loss: 31.79335306141758\n",
            "\n",
            "Epoch: 1000 / 1000 batch step: 100\n",
            "w1: [25.94693808] w2: [-20.83276446] bias: [14.35460618] loss: 31.642888898351682\n",
            "\n",
            "Epoch: 1000 / 1000 batch step: 200\n",
            "w1: [25.99852506] w2: [-20.81883806] bias: [14.426876] loss: 31.46173258602271\n",
            "\n",
            "Epoch: 1000 / 1000 batch step: 300\n",
            "w1: [25.99194781] w2: [-20.8156674] bias: [14.43287791] loss: 31.457346482944246\n",
            "\n",
            "Epoch: 1000 / 1000 batch step: 400\n",
            "w1: [25.96906129] w2: [-20.83058618] bias: [14.39690868] loss: 31.544147617314767\n",
            "\n",
            "Epoch: 1000 / 1000 batch step: 500\n",
            "w1: [25.91522869] w2: [-20.84778858] bias: [14.29793113] loss: 31.791557498037594\n",
            "\n",
            "##### 최종 w1, w2, bias #######\n",
            "[25.91522869] [-20.84778858] [14.29793113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "bostonDF['PREDICTED_PRICE_BATCH'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:14:05.882698Z",
          "iopub.execute_input": "2022-02-10T06:14:05.883735Z",
          "iopub.status.idle": "2022-02-10T06:14:05.953043Z",
          "shell.execute_reply.started": "2022-02-10T06:14:05.8837Z",
          "shell.execute_reply": "2022-02-10T06:14:05.952022Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "wYSudGmOiju6",
        "outputId": "02aec3e2-e121-4a65-b46e-d71e6bdd6632"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-acce5188-95e6-46cf-a144-3d7b4a312905\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE_SGD</th>\n",
              "      <th>PREDICTED_PRICE_BATCH_RANDOM</th>\n",
              "      <th>PREDICTED_PRICE_BATCH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.439906</td>\n",
              "      <td>26.048340</td>\n",
              "      <td>27.394484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>24.394043</td>\n",
              "      <td>24.028449</td>\n",
              "      <td>24.236665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>28.954733</td>\n",
              "      <td>28.567896</td>\n",
              "      <td>30.969979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>28.680936</td>\n",
              "      <td>28.283078</td>\n",
              "      <td>30.668465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>28.351467</td>\n",
              "      <td>27.972934</td>\n",
              "      <td>30.033437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>25.840615</td>\n",
              "      <td>25.447954</td>\n",
              "      <td>26.542167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.752975</td>\n",
              "      <td>21.402594</td>\n",
              "      <td>20.313115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>19.904206</td>\n",
              "      <td>19.603360</td>\n",
              "      <td>17.241789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>14.097085</td>\n",
              "      <td>13.860908</td>\n",
              "      <td>8.354012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>20.043619</td>\n",
              "      <td>19.725452</td>\n",
              "      <td>17.586879</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acce5188-95e6-46cf-a144-3d7b4a312905')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-acce5188-95e6-46cf-a144-3d7b4a312905 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-acce5188-95e6-46cf-a144-3d7b4a312905');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  ...  PREDICTED_PRICE_BATCH_RANDOM  PREDICTED_PRICE_BATCH\n",
              "0  0.00632  18.0  ...                     26.048340              27.394484\n",
              "1  0.02731   0.0  ...                     24.028449              24.236665\n",
              "2  0.02729   0.0  ...                     28.567896              30.969979\n",
              "3  0.03237   0.0  ...                     28.283078              30.668465\n",
              "4  0.06905   0.0  ...                     27.972934              30.033437\n",
              "5  0.02985   0.0  ...                     25.447954              26.542167\n",
              "6  0.08829  12.5  ...                     21.402594              20.313115\n",
              "7  0.14455  12.5  ...                     19.603360              17.241789\n",
              "8  0.21124  12.5  ...                     13.860908               8.354012\n",
              "9  0.17004  12.5  ...                     19.725452              17.586879\n",
              "\n",
              "[10 rows x 17 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini BATCH GD를 Keras로 수행\n",
        "* Keras는 기본적으로 Mini Batch GD를 수행"
      ],
      "metadata": {
        "id": "2WbZrqa7iju6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. \n",
        "    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n",
        "    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n",
        "])\n",
        "# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\n",
        "\n",
        "# Keras는 반드시 Batch GD를 적용함. batch_size가 None이면 32를 할당. \n",
        "model.fit(scaled_features, bostonDF['PRICE'].values, batch_size=100, epochs=1000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-10T06:14:45.613578Z",
          "iopub.execute_input": "2022-02-10T06:14:45.613938Z",
          "iopub.status.idle": "2022-02-10T06:15:01.877524Z",
          "shell.execute_reply.started": "2022-02-10T06:14:45.613902Z",
          "shell.execute_reply": "2022-02-10T06:15:01.876081Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmrX3oDjiju6",
        "outputId": "af1791ac-775c-46bf-e4c9-19e687ee08e2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "6/6 [==============================] - 1s 3ms/step - loss: 546.4967 - mse: 546.4967\n",
            "Epoch 2/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 541.8507 - mse: 541.8507\n",
            "Epoch 3/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 537.3311 - mse: 537.3311\n",
            "Epoch 4/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 532.7287 - mse: 532.7287\n",
            "Epoch 5/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 528.2014 - mse: 528.2014\n",
            "Epoch 6/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 523.6542 - mse: 523.6542\n",
            "Epoch 7/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 519.1941 - mse: 519.1942\n",
            "Epoch 8/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 514.8293 - mse: 514.8293\n",
            "Epoch 9/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 510.4187 - mse: 510.4187\n",
            "Epoch 10/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 506.0229 - mse: 506.0229\n",
            "Epoch 11/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 501.6604 - mse: 501.6604\n",
            "Epoch 12/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 497.3513 - mse: 497.3513\n",
            "Epoch 13/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 493.1469 - mse: 493.1469\n",
            "Epoch 14/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 488.8544 - mse: 488.8544\n",
            "Epoch 15/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 484.6697 - mse: 484.6697\n",
            "Epoch 16/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 480.5451 - mse: 480.5451\n",
            "Epoch 17/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 476.5212 - mse: 476.5212\n",
            "Epoch 18/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 472.5326 - mse: 472.5326\n",
            "Epoch 19/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 468.5433 - mse: 468.5433\n",
            "Epoch 20/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 464.5598 - mse: 464.5598\n",
            "Epoch 21/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 460.6736 - mse: 460.6736\n",
            "Epoch 22/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 456.7711 - mse: 456.7711\n",
            "Epoch 23/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 452.9427 - mse: 452.9427\n",
            "Epoch 24/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 449.0756 - mse: 449.0756\n",
            "Epoch 25/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 445.2520 - mse: 445.2520\n",
            "Epoch 26/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 441.4666 - mse: 441.4666\n",
            "Epoch 27/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 437.6716 - mse: 437.6716\n",
            "Epoch 28/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 433.9398 - mse: 433.9398\n",
            "Epoch 29/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 430.2526 - mse: 430.2526\n",
            "Epoch 30/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 426.4494 - mse: 426.4494\n",
            "Epoch 31/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 422.6452 - mse: 422.6452\n",
            "Epoch 32/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 418.9320 - mse: 418.9320\n",
            "Epoch 33/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 415.2033 - mse: 415.2033\n",
            "Epoch 34/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 411.5819 - mse: 411.5819\n",
            "Epoch 35/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 407.9785 - mse: 407.9785\n",
            "Epoch 36/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 404.4073 - mse: 404.4073\n",
            "Epoch 37/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 400.8049 - mse: 400.8049\n",
            "Epoch 38/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 397.2205 - mse: 397.2205\n",
            "Epoch 39/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 393.7979 - mse: 393.7979\n",
            "Epoch 40/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 390.3889 - mse: 390.3889\n",
            "Epoch 41/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 387.0675 - mse: 387.0675\n",
            "Epoch 42/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 383.7909 - mse: 383.7909\n",
            "Epoch 43/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 380.4812 - mse: 380.4812\n",
            "Epoch 44/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 377.2518 - mse: 377.2518\n",
            "Epoch 45/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 374.1428 - mse: 374.1428\n",
            "Epoch 46/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 371.0198 - mse: 371.0198\n",
            "Epoch 47/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 367.9194 - mse: 367.9194\n",
            "Epoch 48/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 364.7395 - mse: 364.7395\n",
            "Epoch 49/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 361.6193 - mse: 361.6193\n",
            "Epoch 50/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 358.5391 - mse: 358.5391\n",
            "Epoch 51/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 355.3941 - mse: 355.3941\n",
            "Epoch 52/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 352.3267 - mse: 352.3267\n",
            "Epoch 53/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 349.3553 - mse: 349.3553\n",
            "Epoch 54/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 346.3066 - mse: 346.3066\n",
            "Epoch 55/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 343.2878 - mse: 343.2878\n",
            "Epoch 56/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 340.2703 - mse: 340.2703\n",
            "Epoch 57/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 337.2217 - mse: 337.2217\n",
            "Epoch 58/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 334.2504 - mse: 334.2504\n",
            "Epoch 59/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 331.3609 - mse: 331.3609\n",
            "Epoch 60/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 328.5607 - mse: 328.5607\n",
            "Epoch 61/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 325.7350 - mse: 325.7350\n",
            "Epoch 62/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 322.8444 - mse: 322.8444\n",
            "Epoch 63/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 320.0247 - mse: 320.0247\n",
            "Epoch 64/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 317.2639 - mse: 317.2639\n",
            "Epoch 65/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 314.5507 - mse: 314.5507\n",
            "Epoch 66/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 311.9312 - mse: 311.9312\n",
            "Epoch 67/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 309.3384 - mse: 309.3384\n",
            "Epoch 68/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 306.7608 - mse: 306.7608\n",
            "Epoch 69/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 304.1180 - mse: 304.1180\n",
            "Epoch 70/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 301.5311 - mse: 301.5311\n",
            "Epoch 71/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 299.0581 - mse: 299.0581\n",
            "Epoch 72/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 296.5455 - mse: 296.5455\n",
            "Epoch 73/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 294.1492 - mse: 294.1492\n",
            "Epoch 74/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 291.7083 - mse: 291.7083\n",
            "Epoch 75/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 289.2969 - mse: 289.2969\n",
            "Epoch 76/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 286.8857 - mse: 286.8857\n",
            "Epoch 77/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 284.5479 - mse: 284.5479\n",
            "Epoch 78/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 282.2073 - mse: 282.2073\n",
            "Epoch 79/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 279.8646 - mse: 279.8646\n",
            "Epoch 80/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 277.5814 - mse: 277.5814\n",
            "Epoch 81/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 275.3488 - mse: 275.3488\n",
            "Epoch 82/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 273.0829 - mse: 273.0830\n",
            "Epoch 83/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 270.7797 - mse: 270.7797\n",
            "Epoch 84/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 268.4665 - mse: 268.4665\n",
            "Epoch 85/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 266.2538 - mse: 266.2538\n",
            "Epoch 86/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 264.0147 - mse: 264.0147\n",
            "Epoch 87/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 261.7889 - mse: 261.7889\n",
            "Epoch 88/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 259.5625 - mse: 259.5625\n",
            "Epoch 89/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 257.3583 - mse: 257.3583\n",
            "Epoch 90/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 255.2104 - mse: 255.2104\n",
            "Epoch 91/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 253.0957 - mse: 253.0957\n",
            "Epoch 92/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 251.0237 - mse: 251.0237\n",
            "Epoch 93/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 249.0255 - mse: 249.0255\n",
            "Epoch 94/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 246.9951 - mse: 246.9951\n",
            "Epoch 95/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 244.9651 - mse: 244.9651\n",
            "Epoch 96/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 242.9377 - mse: 242.9377\n",
            "Epoch 97/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 240.9754 - mse: 240.9754\n",
            "Epoch 98/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 239.0594 - mse: 239.0594\n",
            "Epoch 99/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 237.2075 - mse: 237.2074\n",
            "Epoch 100/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 235.2804 - mse: 235.2804\n",
            "Epoch 101/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 233.4411 - mse: 233.4411\n",
            "Epoch 102/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 231.5884 - mse: 231.5885\n",
            "Epoch 103/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 229.7720 - mse: 229.7720\n",
            "Epoch 104/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 227.9842 - mse: 227.9842\n",
            "Epoch 105/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 226.2208 - mse: 226.2208\n",
            "Epoch 106/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 224.3902 - mse: 224.3902\n",
            "Epoch 107/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 222.5910 - mse: 222.5910\n",
            "Epoch 108/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 220.7748 - mse: 220.7748\n",
            "Epoch 109/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 218.9960 - mse: 218.9960\n",
            "Epoch 110/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 217.2885 - mse: 217.2885\n",
            "Epoch 111/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 215.5812 - mse: 215.5812\n",
            "Epoch 112/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 213.9520 - mse: 213.9520\n",
            "Epoch 113/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 212.2692 - mse: 212.2692\n",
            "Epoch 114/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 210.5740 - mse: 210.5740\n",
            "Epoch 115/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 208.8710 - mse: 208.8710\n",
            "Epoch 116/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 207.1917 - mse: 207.1917\n",
            "Epoch 117/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 205.5572 - mse: 205.5572\n",
            "Epoch 118/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 203.9461 - mse: 203.9461\n",
            "Epoch 119/1000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 202.4123 - mse: 202.4123\n",
            "Epoch 120/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 200.8346 - mse: 200.8346\n",
            "Epoch 121/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 199.3127 - mse: 199.3127\n",
            "Epoch 122/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 197.8042 - mse: 197.8042\n",
            "Epoch 123/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 196.3248 - mse: 196.3248\n",
            "Epoch 124/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 194.8819 - mse: 194.8819\n",
            "Epoch 125/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 193.4441 - mse: 193.4441\n",
            "Epoch 126/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 192.0053 - mse: 192.0053\n",
            "Epoch 127/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 190.6723 - mse: 190.6723\n",
            "Epoch 128/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 189.3033 - mse: 189.3033\n",
            "Epoch 129/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 187.9911 - mse: 187.9911\n",
            "Epoch 130/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 186.6156 - mse: 186.6156\n",
            "Epoch 131/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 185.3105 - mse: 185.3105\n",
            "Epoch 132/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 183.9686 - mse: 183.9686\n",
            "Epoch 133/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 182.7020 - mse: 182.7020\n",
            "Epoch 134/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 181.3969 - mse: 181.3969\n",
            "Epoch 135/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 180.1460 - mse: 180.1460\n",
            "Epoch 136/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 178.9223 - mse: 178.9223\n",
            "Epoch 137/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 177.6748 - mse: 177.6748\n",
            "Epoch 138/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 176.4637 - mse: 176.4637\n",
            "Epoch 139/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 175.1804 - mse: 175.1804\n",
            "Epoch 140/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 173.9438 - mse: 173.9438\n",
            "Epoch 141/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 172.6786 - mse: 172.6786\n",
            "Epoch 142/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 171.5015 - mse: 171.5016\n",
            "Epoch 143/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 170.3041 - mse: 170.3041\n",
            "Epoch 144/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 169.1532 - mse: 169.1532\n",
            "Epoch 145/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 168.0406 - mse: 168.0406\n",
            "Epoch 146/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 166.8647 - mse: 166.8647\n",
            "Epoch 147/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 165.7163 - mse: 165.7163\n",
            "Epoch 148/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 164.5817 - mse: 164.5817\n",
            "Epoch 149/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 163.4197 - mse: 163.4197\n",
            "Epoch 150/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 162.2888 - mse: 162.2888\n",
            "Epoch 151/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 161.1677 - mse: 161.1677\n",
            "Epoch 152/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 160.0557 - mse: 160.0557\n",
            "Epoch 153/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 159.0076 - mse: 159.0076\n",
            "Epoch 154/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 157.9742 - mse: 157.9742\n",
            "Epoch 155/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 156.9840 - mse: 156.9840\n",
            "Epoch 156/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 155.9633 - mse: 155.9633\n",
            "Epoch 157/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 154.9470 - mse: 154.9470\n",
            "Epoch 158/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 153.9242 - mse: 153.9242\n",
            "Epoch 159/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 152.9516 - mse: 152.9516\n",
            "Epoch 160/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 151.9719 - mse: 151.9719\n",
            "Epoch 161/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 151.0294 - mse: 151.0294\n",
            "Epoch 162/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 150.0930 - mse: 150.0930\n",
            "Epoch 163/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 149.1537 - mse: 149.1537\n",
            "Epoch 164/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 148.2473 - mse: 148.2473\n",
            "Epoch 165/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 147.3359 - mse: 147.3359\n",
            "Epoch 166/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 146.4459 - mse: 146.4459\n",
            "Epoch 167/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 145.5825 - mse: 145.5825\n",
            "Epoch 168/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 144.7445 - mse: 144.7445\n",
            "Epoch 169/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 143.8872 - mse: 143.8872\n",
            "Epoch 170/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 143.0631 - mse: 143.0631\n",
            "Epoch 171/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 142.2209 - mse: 142.2209\n",
            "Epoch 172/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 141.3728 - mse: 141.3728\n",
            "Epoch 173/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 140.5311 - mse: 140.5311\n",
            "Epoch 174/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 139.7223 - mse: 139.7223\n",
            "Epoch 175/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 138.8898 - mse: 138.8898\n",
            "Epoch 176/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 138.1209 - mse: 138.1209\n",
            "Epoch 177/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 137.3447 - mse: 137.3447\n",
            "Epoch 178/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 136.5832 - mse: 136.5832\n",
            "Epoch 179/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 135.7854 - mse: 135.7854\n",
            "Epoch 180/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 134.9633 - mse: 134.9633\n",
            "Epoch 181/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 134.2186 - mse: 134.2186\n",
            "Epoch 182/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 133.4861 - mse: 133.4861\n",
            "Epoch 183/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 132.7696 - mse: 132.7696\n",
            "Epoch 184/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 132.0503 - mse: 132.0503\n",
            "Epoch 185/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 131.3378 - mse: 131.3378\n",
            "Epoch 186/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 130.6636 - mse: 130.6636\n",
            "Epoch 187/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 130.0183 - mse: 130.0183\n",
            "Epoch 188/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 129.3602 - mse: 129.3602\n",
            "Epoch 189/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 128.7032 - mse: 128.7032\n",
            "Epoch 190/1000\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 128.0412 - mse: 128.0412\n",
            "Epoch 191/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 127.3393 - mse: 127.3393\n",
            "Epoch 192/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 126.6547 - mse: 126.6547\n",
            "Epoch 193/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 125.9921 - mse: 125.9921\n",
            "Epoch 194/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 125.3689 - mse: 125.3689\n",
            "Epoch 195/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 124.7431 - mse: 124.7431\n",
            "Epoch 196/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 124.1379 - mse: 124.1379\n",
            "Epoch 197/1000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 123.5380 - mse: 123.5380\n",
            "Epoch 198/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 122.9661 - mse: 122.9661\n",
            "Epoch 199/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 122.3872 - mse: 122.3872\n",
            "Epoch 200/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 121.8505 - mse: 121.8505\n",
            "Epoch 201/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 121.2828 - mse: 121.2828\n",
            "Epoch 202/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 120.6731 - mse: 120.6731\n",
            "Epoch 203/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 120.0786 - mse: 120.0786\n",
            "Epoch 204/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 119.4378 - mse: 119.4378\n",
            "Epoch 205/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 118.8626 - mse: 118.8626\n",
            "Epoch 206/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 118.3103 - mse: 118.3103\n",
            "Epoch 207/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 117.7736 - mse: 117.7736\n",
            "Epoch 208/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 117.2229 - mse: 117.2229\n",
            "Epoch 209/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 116.7121 - mse: 116.7121\n",
            "Epoch 210/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 116.1997 - mse: 116.1997\n",
            "Epoch 211/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 115.6717 - mse: 115.6717\n",
            "Epoch 212/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 115.1851 - mse: 115.1851\n",
            "Epoch 213/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 114.7227 - mse: 114.7227\n",
            "Epoch 214/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 114.2993 - mse: 114.2993\n",
            "Epoch 215/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 113.8272 - mse: 113.8272\n",
            "Epoch 216/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 113.3770 - mse: 113.3770\n",
            "Epoch 217/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 112.8847 - mse: 112.8847\n",
            "Epoch 218/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 112.4175 - mse: 112.4175\n",
            "Epoch 219/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 111.9828 - mse: 111.9828\n",
            "Epoch 220/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 111.5263 - mse: 111.5263\n",
            "Epoch 221/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 111.0692 - mse: 111.0692\n",
            "Epoch 222/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 110.6174 - mse: 110.6174\n",
            "Epoch 223/1000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 110.2199 - mse: 110.2199\n",
            "Epoch 224/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 109.8053 - mse: 109.8052\n",
            "Epoch 225/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 109.4003 - mse: 109.4003\n",
            "Epoch 226/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 108.9350 - mse: 108.9350\n",
            "Epoch 227/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 108.4945 - mse: 108.4945\n",
            "Epoch 228/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 108.0714 - mse: 108.0714\n",
            "Epoch 229/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 107.6769 - mse: 107.6769\n",
            "Epoch 230/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 107.2635 - mse: 107.2635\n",
            "Epoch 231/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 106.8792 - mse: 106.8792\n",
            "Epoch 232/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 106.4613 - mse: 106.4613\n",
            "Epoch 233/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 106.0585 - mse: 106.0585\n",
            "Epoch 234/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 105.6674 - mse: 105.6674\n",
            "Epoch 235/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 105.2304 - mse: 105.2304\n",
            "Epoch 236/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 104.8380 - mse: 104.8380\n",
            "Epoch 237/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 104.4611 - mse: 104.4611\n",
            "Epoch 238/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 104.0742 - mse: 104.0742\n",
            "Epoch 239/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 103.7278 - mse: 103.7278\n",
            "Epoch 240/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 103.3859 - mse: 103.3859\n",
            "Epoch 241/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 103.0233 - mse: 103.0233\n",
            "Epoch 242/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 102.6638 - mse: 102.6638\n",
            "Epoch 243/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 102.3287 - mse: 102.3287\n",
            "Epoch 244/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 101.9921 - mse: 101.9921\n",
            "Epoch 245/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 101.6900 - mse: 101.6900\n",
            "Epoch 246/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 101.3958 - mse: 101.3958\n",
            "Epoch 247/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 101.0947 - mse: 101.0947\n",
            "Epoch 248/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 100.8392 - mse: 100.8392\n",
            "Epoch 249/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 100.5804 - mse: 100.5804\n",
            "Epoch 250/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 100.3295 - mse: 100.3295\n",
            "Epoch 251/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 100.0709 - mse: 100.0709\n",
            "Epoch 252/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 99.7915 - mse: 99.7915\n",
            "Epoch 253/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 99.5021 - mse: 99.5021\n",
            "Epoch 254/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 99.1907 - mse: 99.1907\n",
            "Epoch 255/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 98.9098 - mse: 98.9098\n",
            "Epoch 256/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 98.6431 - mse: 98.6431\n",
            "Epoch 257/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 98.3638 - mse: 98.3638\n",
            "Epoch 258/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 98.0607 - mse: 98.0607\n",
            "Epoch 259/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 97.7848 - mse: 97.7848\n",
            "Epoch 260/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 97.5051 - mse: 97.5051\n",
            "Epoch 261/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 97.2024 - mse: 97.2024\n",
            "Epoch 262/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 96.9234 - mse: 96.9234\n",
            "Epoch 263/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 96.6280 - mse: 96.6280\n",
            "Epoch 264/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 96.3339 - mse: 96.3339\n",
            "Epoch 265/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 96.0377 - mse: 96.0377\n",
            "Epoch 266/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 95.7499 - mse: 95.7499\n",
            "Epoch 267/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 95.4794 - mse: 95.4794\n",
            "Epoch 268/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 95.2137 - mse: 95.2137\n",
            "Epoch 269/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 94.9373 - mse: 94.9373\n",
            "Epoch 270/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 94.6772 - mse: 94.6772\n",
            "Epoch 271/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 94.4133 - mse: 94.4133\n",
            "Epoch 272/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 94.1440 - mse: 94.1440\n",
            "Epoch 273/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 93.8960 - mse: 93.8960\n",
            "Epoch 274/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 93.6623 - mse: 93.6623\n",
            "Epoch 275/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 93.4179 - mse: 93.4179\n",
            "Epoch 276/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 93.1723 - mse: 93.1723\n",
            "Epoch 277/1000\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 92.9143 - mse: 92.9143\n",
            "Epoch 278/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 92.6633 - mse: 92.6633\n",
            "Epoch 279/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 92.4345 - mse: 92.4345\n",
            "Epoch 280/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 92.2082 - mse: 92.2082\n",
            "Epoch 281/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 91.9757 - mse: 91.9757\n",
            "Epoch 282/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 91.7141 - mse: 91.7141\n",
            "Epoch 283/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 91.4489 - mse: 91.4489\n",
            "Epoch 284/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 91.1892 - mse: 91.1892\n",
            "Epoch 285/1000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 90.9326 - mse: 90.9326\n",
            "Epoch 286/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 90.6873 - mse: 90.6873\n",
            "Epoch 287/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 90.4374 - mse: 90.4374\n",
            "Epoch 288/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 90.2081 - mse: 90.2081\n",
            "Epoch 289/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 89.9999 - mse: 89.9999\n",
            "Epoch 290/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 89.7989 - mse: 89.7989\n",
            "Epoch 291/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 89.5953 - mse: 89.5953\n",
            "Epoch 292/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 89.3930 - mse: 89.3930\n",
            "Epoch 293/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 89.1892 - mse: 89.1892\n",
            "Epoch 294/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 88.9958 - mse: 88.9958\n",
            "Epoch 295/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 88.7819 - mse: 88.7819\n",
            "Epoch 296/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.5843 - mse: 88.5843\n",
            "Epoch 297/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 88.3666 - mse: 88.3666\n",
            "Epoch 298/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 88.1563 - mse: 88.1563\n",
            "Epoch 299/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.9411 - mse: 87.9411\n",
            "Epoch 300/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 87.7378 - mse: 87.7378\n",
            "Epoch 301/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 87.5432 - mse: 87.5432\n",
            "Epoch 302/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 87.3437 - mse: 87.3437\n",
            "Epoch 303/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 87.1379 - mse: 87.1379\n",
            "Epoch 304/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 86.9513 - mse: 86.9513\n",
            "Epoch 305/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 86.7767 - mse: 86.7767\n",
            "Epoch 306/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 86.6020 - mse: 86.6020\n",
            "Epoch 307/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 86.4125 - mse: 86.4125\n",
            "Epoch 308/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 86.2334 - mse: 86.2334\n",
            "Epoch 309/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 86.0551 - mse: 86.0551\n",
            "Epoch 310/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 85.8693 - mse: 85.8693\n",
            "Epoch 311/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 85.6873 - mse: 85.6873\n",
            "Epoch 312/1000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 85.4960 - mse: 85.4960\n",
            "Epoch 313/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 85.3191 - mse: 85.3191\n",
            "Epoch 314/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 85.1413 - mse: 85.1413\n",
            "Epoch 315/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 84.9625 - mse: 84.9625\n",
            "Epoch 316/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 84.7911 - mse: 84.7911\n",
            "Epoch 317/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 84.6211 - mse: 84.6211\n",
            "Epoch 318/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 84.4515 - mse: 84.4515\n",
            "Epoch 319/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 84.2903 - mse: 84.2903\n",
            "Epoch 320/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 84.1269 - mse: 84.1269\n",
            "Epoch 321/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 83.9620 - mse: 83.9620\n",
            "Epoch 322/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 83.7931 - mse: 83.7931\n",
            "Epoch 323/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 83.6225 - mse: 83.6225\n",
            "Epoch 324/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 83.4492 - mse: 83.4492\n",
            "Epoch 325/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 83.2555 - mse: 83.2555\n",
            "Epoch 326/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 83.0805 - mse: 83.0805\n",
            "Epoch 327/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 82.9155 - mse: 82.9155\n",
            "Epoch 328/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 82.7572 - mse: 82.7572\n",
            "Epoch 329/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 82.6055 - mse: 82.6055\n",
            "Epoch 330/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 82.4446 - mse: 82.4446\n",
            "Epoch 331/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 82.2663 - mse: 82.2663\n",
            "Epoch 332/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 82.0978 - mse: 82.0978\n",
            "Epoch 333/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 81.9332 - mse: 81.9332\n",
            "Epoch 334/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 81.7684 - mse: 81.7684\n",
            "Epoch 335/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 81.6088 - mse: 81.6088\n",
            "Epoch 336/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 81.4473 - mse: 81.4473\n",
            "Epoch 337/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 81.2863 - mse: 81.2863\n",
            "Epoch 338/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 81.1175 - mse: 81.1175\n",
            "Epoch 339/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 80.9471 - mse: 80.9471\n",
            "Epoch 340/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 80.7799 - mse: 80.7799\n",
            "Epoch 341/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 80.5970 - mse: 80.5970\n",
            "Epoch 342/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 80.4346 - mse: 80.4346\n",
            "Epoch 343/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 80.2604 - mse: 80.2604\n",
            "Epoch 344/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 80.0976 - mse: 80.0976\n",
            "Epoch 345/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 79.9513 - mse: 79.9513\n",
            "Epoch 346/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 79.7945 - mse: 79.7945\n",
            "Epoch 347/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 79.6292 - mse: 79.6292\n",
            "Epoch 348/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 79.4740 - mse: 79.4740\n",
            "Epoch 349/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 79.3221 - mse: 79.3221\n",
            "Epoch 350/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 79.1819 - mse: 79.1819\n",
            "Epoch 351/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 79.0337 - mse: 79.0337\n",
            "Epoch 352/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 78.8851 - mse: 78.8851\n",
            "Epoch 353/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 78.7448 - mse: 78.7448\n",
            "Epoch 354/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 78.5939 - mse: 78.5939\n",
            "Epoch 355/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 78.4594 - mse: 78.4594\n",
            "Epoch 356/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 78.3182 - mse: 78.3182\n",
            "Epoch 357/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 78.1815 - mse: 78.1815\n",
            "Epoch 358/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 78.0390 - mse: 78.0390\n",
            "Epoch 359/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 77.8872 - mse: 77.8872\n",
            "Epoch 360/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 77.7341 - mse: 77.7341\n",
            "Epoch 361/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 77.5882 - mse: 77.5882\n",
            "Epoch 362/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 77.4533 - mse: 77.4533\n",
            "Epoch 363/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 77.3093 - mse: 77.3093\n",
            "Epoch 364/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 77.1613 - mse: 77.1613\n",
            "Epoch 365/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 77.0118 - mse: 77.0118\n",
            "Epoch 366/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 76.8757 - mse: 76.8757\n",
            "Epoch 367/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 76.7347 - mse: 76.7347\n",
            "Epoch 368/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 76.5976 - mse: 76.5976\n",
            "Epoch 369/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 76.4537 - mse: 76.4537\n",
            "Epoch 370/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 76.3095 - mse: 76.3095\n",
            "Epoch 371/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 76.1632 - mse: 76.1632\n",
            "Epoch 372/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 76.0158 - mse: 76.0157\n",
            "Epoch 373/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 75.8732 - mse: 75.8732\n",
            "Epoch 374/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 75.7190 - mse: 75.7190\n",
            "Epoch 375/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 75.5787 - mse: 75.5787\n",
            "Epoch 376/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 75.4264 - mse: 75.4264\n",
            "Epoch 377/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 75.2890 - mse: 75.2890\n",
            "Epoch 378/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 75.1347 - mse: 75.1347\n",
            "Epoch 379/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 74.9977 - mse: 74.9977\n",
            "Epoch 380/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 74.8616 - mse: 74.8616\n",
            "Epoch 381/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 74.7287 - mse: 74.7287\n",
            "Epoch 382/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 74.5861 - mse: 74.5861\n",
            "Epoch 383/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 74.4497 - mse: 74.4497\n",
            "Epoch 384/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 74.3096 - mse: 74.3096\n",
            "Epoch 385/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 74.1547 - mse: 74.1547\n",
            "Epoch 386/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 73.9879 - mse: 73.9879\n",
            "Epoch 387/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 73.8334 - mse: 73.8334\n",
            "Epoch 388/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 73.6695 - mse: 73.6695\n",
            "Epoch 389/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 73.5247 - mse: 73.5247\n",
            "Epoch 390/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 73.3799 - mse: 73.3799\n",
            "Epoch 391/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 73.2453 - mse: 73.2453\n",
            "Epoch 392/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 73.1075 - mse: 73.1075\n",
            "Epoch 393/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 72.9712 - mse: 72.9712\n",
            "Epoch 394/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 72.8315 - mse: 72.8315\n",
            "Epoch 395/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 72.6894 - mse: 72.6894\n",
            "Epoch 396/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 72.5573 - mse: 72.5573\n",
            "Epoch 397/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 72.4168 - mse: 72.4168\n",
            "Epoch 398/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 72.2742 - mse: 72.2742\n",
            "Epoch 399/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 72.1305 - mse: 72.1305\n",
            "Epoch 400/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 71.9967 - mse: 71.9967\n",
            "Epoch 401/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 71.8525 - mse: 71.8525\n",
            "Epoch 402/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 71.7100 - mse: 71.7100\n",
            "Epoch 403/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 71.5725 - mse: 71.5725\n",
            "Epoch 404/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 71.4412 - mse: 71.4412\n",
            "Epoch 405/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 71.2972 - mse: 71.2972\n",
            "Epoch 406/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 71.1515 - mse: 71.1515\n",
            "Epoch 407/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 71.0075 - mse: 71.0075\n",
            "Epoch 408/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 70.8703 - mse: 70.8703\n",
            "Epoch 409/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 70.7436 - mse: 70.7436\n",
            "Epoch 410/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 70.6178 - mse: 70.6178\n",
            "Epoch 411/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 70.4859 - mse: 70.4859\n",
            "Epoch 412/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 70.3412 - mse: 70.3412\n",
            "Epoch 413/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 70.1997 - mse: 70.1997\n",
            "Epoch 414/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 70.0367 - mse: 70.0367\n",
            "Epoch 415/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 69.8927 - mse: 69.8927\n",
            "Epoch 416/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 69.7585 - mse: 69.7585\n",
            "Epoch 417/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 69.6287 - mse: 69.6287\n",
            "Epoch 418/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 69.5005 - mse: 69.5005\n",
            "Epoch 419/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 69.3728 - mse: 69.3728\n",
            "Epoch 420/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 69.2447 - mse: 69.2447\n",
            "Epoch 421/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 69.1245 - mse: 69.1245\n",
            "Epoch 422/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 68.9942 - mse: 68.9942\n",
            "Epoch 423/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 68.8653 - mse: 68.8653\n",
            "Epoch 424/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 68.7302 - mse: 68.7302\n",
            "Epoch 425/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 68.6011 - mse: 68.6011\n",
            "Epoch 426/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 68.4590 - mse: 68.4590\n",
            "Epoch 427/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 68.3168 - mse: 68.3168\n",
            "Epoch 428/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 68.1690 - mse: 68.1690\n",
            "Epoch 429/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 68.0313 - mse: 68.0313\n",
            "Epoch 430/1000\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 67.8993 - mse: 67.8993\n",
            "Epoch 431/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 67.7739 - mse: 67.7739\n",
            "Epoch 432/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 67.6454 - mse: 67.6454\n",
            "Epoch 433/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 67.5260 - mse: 67.5260\n",
            "Epoch 434/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 67.3892 - mse: 67.3892\n",
            "Epoch 435/1000\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 67.2607 - mse: 67.2607\n",
            "Epoch 436/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 67.1277 - mse: 67.1277\n",
            "Epoch 437/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 67.0091 - mse: 67.0091\n",
            "Epoch 438/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 66.8868 - mse: 66.8868\n",
            "Epoch 439/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 66.7602 - mse: 66.7602\n",
            "Epoch 440/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 66.6349 - mse: 66.6349\n",
            "Epoch 441/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 66.5118 - mse: 66.5118\n",
            "Epoch 442/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 66.3890 - mse: 66.3890\n",
            "Epoch 443/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 66.2638 - mse: 66.2638\n",
            "Epoch 444/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 66.1406 - mse: 66.1406\n",
            "Epoch 445/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 66.0202 - mse: 66.0202\n",
            "Epoch 446/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 65.8959 - mse: 65.8959\n",
            "Epoch 447/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 65.7795 - mse: 65.7795\n",
            "Epoch 448/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 65.6667 - mse: 65.6667\n",
            "Epoch 449/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 65.5526 - mse: 65.5526\n",
            "Epoch 450/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 65.4405 - mse: 65.4405\n",
            "Epoch 451/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 65.3321 - mse: 65.3321\n",
            "Epoch 452/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 65.2199 - mse: 65.2199\n",
            "Epoch 453/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 65.1079 - mse: 65.1079\n",
            "Epoch 454/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 64.9930 - mse: 64.9930\n",
            "Epoch 455/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 64.8750 - mse: 64.8750\n",
            "Epoch 456/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 64.7522 - mse: 64.7522\n",
            "Epoch 457/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 64.6311 - mse: 64.6311\n",
            "Epoch 458/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 64.5147 - mse: 64.5147\n",
            "Epoch 459/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 64.3989 - mse: 64.3989\n",
            "Epoch 460/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 64.2777 - mse: 64.2776\n",
            "Epoch 461/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 64.1588 - mse: 64.1588\n",
            "Epoch 462/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 64.0423 - mse: 64.0423\n",
            "Epoch 463/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 63.9298 - mse: 63.9298\n",
            "Epoch 464/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 63.8215 - mse: 63.8215\n",
            "Epoch 465/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 63.7094 - mse: 63.7094\n",
            "Epoch 466/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 63.5954 - mse: 63.5954\n",
            "Epoch 467/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 63.4816 - mse: 63.4816\n",
            "Epoch 468/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 63.3580 - mse: 63.3580\n",
            "Epoch 469/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 63.2368 - mse: 63.2368\n",
            "Epoch 470/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 63.1105 - mse: 63.1105\n",
            "Epoch 471/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 62.9848 - mse: 62.9848\n",
            "Epoch 472/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 62.8623 - mse: 62.8623\n",
            "Epoch 473/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 62.7423 - mse: 62.7423\n",
            "Epoch 474/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 62.6255 - mse: 62.6255\n",
            "Epoch 475/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 62.5153 - mse: 62.5153\n",
            "Epoch 476/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 62.4023 - mse: 62.4023\n",
            "Epoch 477/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 62.2940 - mse: 62.2940\n",
            "Epoch 478/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 62.1840 - mse: 62.1840\n",
            "Epoch 479/1000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 62.0774 - mse: 62.0774\n",
            "Epoch 480/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 61.9700 - mse: 61.9700\n",
            "Epoch 481/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 61.8642 - mse: 61.8642\n",
            "Epoch 482/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 61.7586 - mse: 61.7586\n",
            "Epoch 483/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 61.6561 - mse: 61.6561\n",
            "Epoch 484/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 61.5512 - mse: 61.5512\n",
            "Epoch 485/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 61.4521 - mse: 61.4521\n",
            "Epoch 486/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 61.3393 - mse: 61.3393\n",
            "Epoch 487/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 61.2222 - mse: 61.2222\n",
            "Epoch 488/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 61.1123 - mse: 61.1123\n",
            "Epoch 489/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 60.9940 - mse: 60.9940\n",
            "Epoch 490/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 60.8805 - mse: 60.8805\n",
            "Epoch 491/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 60.7641 - mse: 60.7641\n",
            "Epoch 492/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 60.6567 - mse: 60.6567\n",
            "Epoch 493/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 60.5485 - mse: 60.5485\n",
            "Epoch 494/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 60.4377 - mse: 60.4377\n",
            "Epoch 495/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 60.3168 - mse: 60.3168\n",
            "Epoch 496/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 60.1956 - mse: 60.1956\n",
            "Epoch 497/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 60.0912 - mse: 60.0912\n",
            "Epoch 498/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 59.9808 - mse: 59.9808\n",
            "Epoch 499/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 59.8702 - mse: 59.8702\n",
            "Epoch 500/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 59.7586 - mse: 59.7587\n",
            "Epoch 501/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 59.6399 - mse: 59.6399\n",
            "Epoch 502/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 59.5315 - mse: 59.5315\n",
            "Epoch 503/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 59.4213 - mse: 59.4213\n",
            "Epoch 504/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 59.3136 - mse: 59.3136\n",
            "Epoch 505/1000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 59.2094 - mse: 59.2094\n",
            "Epoch 506/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 59.1009 - mse: 59.1009\n",
            "Epoch 507/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 58.9857 - mse: 58.9857\n",
            "Epoch 508/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 58.8741 - mse: 58.8741\n",
            "Epoch 509/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 58.7672 - mse: 58.7672\n",
            "Epoch 510/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 58.6659 - mse: 58.6659\n",
            "Epoch 511/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 58.5647 - mse: 58.5647\n",
            "Epoch 512/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 58.4607 - mse: 58.4607\n",
            "Epoch 513/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 58.3552 - mse: 58.3552\n",
            "Epoch 514/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 58.2589 - mse: 58.2589\n",
            "Epoch 515/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 58.1562 - mse: 58.1562\n",
            "Epoch 516/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 58.0549 - mse: 58.0549\n",
            "Epoch 517/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 57.9447 - mse: 57.9447\n",
            "Epoch 518/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 57.8385 - mse: 57.8385\n",
            "Epoch 519/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 57.7322 - mse: 57.7322\n",
            "Epoch 520/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 57.6269 - mse: 57.6269\n",
            "Epoch 521/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 57.5094 - mse: 57.5094\n",
            "Epoch 522/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 57.3875 - mse: 57.3875\n",
            "Epoch 523/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 57.2678 - mse: 57.2678\n",
            "Epoch 524/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 57.1598 - mse: 57.1598\n",
            "Epoch 525/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 57.0557 - mse: 57.0557\n",
            "Epoch 526/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 56.9563 - mse: 56.9563\n",
            "Epoch 527/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 56.8486 - mse: 56.8486\n",
            "Epoch 528/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 56.7334 - mse: 56.7334\n",
            "Epoch 529/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 56.6210 - mse: 56.6210\n",
            "Epoch 530/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 56.5188 - mse: 56.5188\n",
            "Epoch 531/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 56.4152 - mse: 56.4152\n",
            "Epoch 532/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 56.3219 - mse: 56.3219\n",
            "Epoch 533/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 56.2226 - mse: 56.2226\n",
            "Epoch 534/1000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 56.1294 - mse: 56.1294\n",
            "Epoch 535/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 56.0351 - mse: 56.0351\n",
            "Epoch 536/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 55.9403 - mse: 55.9403\n",
            "Epoch 537/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 55.8462 - mse: 55.8462\n",
            "Epoch 538/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 55.7480 - mse: 55.7480\n",
            "Epoch 539/1000\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 55.6499 - mse: 55.6499\n",
            "Epoch 540/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 55.5481 - mse: 55.5481\n",
            "Epoch 541/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 55.4458 - mse: 55.4458\n",
            "Epoch 542/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 55.3439 - mse: 55.3439\n",
            "Epoch 543/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 55.2427 - mse: 55.2427\n",
            "Epoch 544/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 55.1461 - mse: 55.1461\n",
            "Epoch 545/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 55.0450 - mse: 55.0450\n",
            "Epoch 546/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 54.9487 - mse: 54.9487\n",
            "Epoch 547/1000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 54.8536 - mse: 54.8536\n",
            "Epoch 548/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 54.7504 - mse: 54.7504\n",
            "Epoch 549/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 54.6593 - mse: 54.6593\n",
            "Epoch 550/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 54.5644 - mse: 54.5644\n",
            "Epoch 551/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 54.4635 - mse: 54.4635\n",
            "Epoch 552/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 54.3683 - mse: 54.3683\n",
            "Epoch 553/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 54.2668 - mse: 54.2668\n",
            "Epoch 554/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 54.1649 - mse: 54.1649\n",
            "Epoch 555/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 54.0636 - mse: 54.0636\n",
            "Epoch 556/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 53.9641 - mse: 53.9641\n",
            "Epoch 557/1000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 53.8611 - mse: 53.8611\n",
            "Epoch 558/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 53.7626 - mse: 53.7626\n",
            "Epoch 559/1000\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 53.6506 - mse: 53.6506\n",
            "Epoch 560/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 53.5358 - mse: 53.5358\n",
            "Epoch 561/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 53.4273 - mse: 53.4273\n",
            "Epoch 562/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 53.3277 - mse: 53.3277\n",
            "Epoch 563/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 53.2121 - mse: 53.2121\n",
            "Epoch 564/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 53.0886 - mse: 53.0886\n",
            "Epoch 565/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.9837 - mse: 52.9837\n",
            "Epoch 566/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.8827 - mse: 52.8827\n",
            "Epoch 567/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.7837 - mse: 52.7837\n",
            "Epoch 568/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.6966 - mse: 52.6966\n",
            "Epoch 569/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 52.6060 - mse: 52.6060\n",
            "Epoch 570/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.5063 - mse: 52.5063\n",
            "Epoch 571/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.4176 - mse: 52.4176\n",
            "Epoch 572/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.3231 - mse: 52.3231\n",
            "Epoch 573/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.2228 - mse: 52.2228\n",
            "Epoch 574/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 52.1358 - mse: 52.1358\n",
            "Epoch 575/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 52.0409 - mse: 52.0409\n",
            "Epoch 576/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.9532 - mse: 51.9532\n",
            "Epoch 577/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.8625 - mse: 51.8625\n",
            "Epoch 578/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.7696 - mse: 51.7696\n",
            "Epoch 579/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.6832 - mse: 51.6832\n",
            "Epoch 580/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 51.5945 - mse: 51.5945\n",
            "Epoch 581/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.5119 - mse: 51.5119\n",
            "Epoch 582/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 51.4242 - mse: 51.4242\n",
            "Epoch 583/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.3382 - mse: 51.3382\n",
            "Epoch 584/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.2422 - mse: 51.2422\n",
            "Epoch 585/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.1426 - mse: 51.1426\n",
            "Epoch 586/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 51.0544 - mse: 51.0544\n",
            "Epoch 587/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.9697 - mse: 50.9697\n",
            "Epoch 588/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.8832 - mse: 50.8832\n",
            "Epoch 589/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.7943 - mse: 50.7943\n",
            "Epoch 590/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 50.7083 - mse: 50.7083\n",
            "Epoch 591/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.6170 - mse: 50.6170\n",
            "Epoch 592/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.5224 - mse: 50.5224\n",
            "Epoch 593/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.4270 - mse: 50.4270\n",
            "Epoch 594/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.3334 - mse: 50.3334\n",
            "Epoch 595/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.2403 - mse: 50.2403\n",
            "Epoch 596/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.1469 - mse: 50.1469\n",
            "Epoch 597/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 50.0606 - mse: 50.0606\n",
            "Epoch 598/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 49.9717 - mse: 49.9717\n",
            "Epoch 599/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 49.8771 - mse: 49.8771\n",
            "Epoch 600/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 49.7954 - mse: 49.7954\n",
            "Epoch 601/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 49.7078 - mse: 49.7078\n",
            "Epoch 602/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 49.6206 - mse: 49.6206\n",
            "Epoch 603/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 49.5316 - mse: 49.5316\n",
            "Epoch 604/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 49.4349 - mse: 49.4349\n",
            "Epoch 605/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 49.3468 - mse: 49.3468\n",
            "Epoch 606/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 49.2559 - mse: 49.2559\n",
            "Epoch 607/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 49.1687 - mse: 49.1687\n",
            "Epoch 608/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 49.0833 - mse: 49.0833\n",
            "Epoch 609/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.9951 - mse: 48.9951\n",
            "Epoch 610/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.9061 - mse: 48.9061\n",
            "Epoch 611/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 48.8206 - mse: 48.8206\n",
            "Epoch 612/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.7309 - mse: 48.7309\n",
            "Epoch 613/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.6420 - mse: 48.6420\n",
            "Epoch 614/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.5528 - mse: 48.5528\n",
            "Epoch 615/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.4616 - mse: 48.4616\n",
            "Epoch 616/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.3744 - mse: 48.3744\n",
            "Epoch 617/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 48.2881 - mse: 48.2881\n",
            "Epoch 618/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 48.2081 - mse: 48.2081\n",
            "Epoch 619/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 48.1252 - mse: 48.1252\n",
            "Epoch 620/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 48.0494 - mse: 48.0494\n",
            "Epoch 621/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.9664 - mse: 47.9664\n",
            "Epoch 622/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 47.8862 - mse: 47.8862\n",
            "Epoch 623/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.8184 - mse: 47.8184\n",
            "Epoch 624/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.7352 - mse: 47.7352\n",
            "Epoch 625/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.6523 - mse: 47.6523\n",
            "Epoch 626/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.5754 - mse: 47.5754\n",
            "Epoch 627/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 47.4971 - mse: 47.4971\n",
            "Epoch 628/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.4133 - mse: 47.4133\n",
            "Epoch 629/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 47.3300 - mse: 47.3300\n",
            "Epoch 630/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 47.2526 - mse: 47.2526\n",
            "Epoch 631/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.1741 - mse: 47.1741\n",
            "Epoch 632/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 47.0969 - mse: 47.0969\n",
            "Epoch 633/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 47.0196 - mse: 47.0196\n",
            "Epoch 634/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.9420 - mse: 46.9420\n",
            "Epoch 635/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 46.8698 - mse: 46.8698\n",
            "Epoch 636/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 46.7962 - mse: 46.7962\n",
            "Epoch 637/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.7202 - mse: 46.7202\n",
            "Epoch 638/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.6484 - mse: 46.6484\n",
            "Epoch 639/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 46.5742 - mse: 46.5742\n",
            "Epoch 640/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.5042 - mse: 46.5042\n",
            "Epoch 641/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.4352 - mse: 46.4352\n",
            "Epoch 642/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 46.3654 - mse: 46.3654\n",
            "Epoch 643/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.2987 - mse: 46.2987\n",
            "Epoch 644/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.2322 - mse: 46.2322\n",
            "Epoch 645/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.1662 - mse: 46.1662\n",
            "Epoch 646/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.0972 - mse: 46.0972\n",
            "Epoch 647/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 46.0266 - mse: 46.0266\n",
            "Epoch 648/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.9523 - mse: 45.9523\n",
            "Epoch 649/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.8859 - mse: 45.8859\n",
            "Epoch 650/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.8084 - mse: 45.8084\n",
            "Epoch 651/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.7332 - mse: 45.7332\n",
            "Epoch 652/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.6689 - mse: 45.6689\n",
            "Epoch 653/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.5986 - mse: 45.5986\n",
            "Epoch 654/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.5266 - mse: 45.5266\n",
            "Epoch 655/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.4591 - mse: 45.4591\n",
            "Epoch 656/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.3836 - mse: 45.3836\n",
            "Epoch 657/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.3079 - mse: 45.3079\n",
            "Epoch 658/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.2264 - mse: 45.2264\n",
            "Epoch 659/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.1599 - mse: 45.1599\n",
            "Epoch 660/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.0796 - mse: 45.0796\n",
            "Epoch 661/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 45.0133 - mse: 45.0133\n",
            "Epoch 662/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.9410 - mse: 44.9410\n",
            "Epoch 663/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 44.8736 - mse: 44.8736\n",
            "Epoch 664/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.8083 - mse: 44.8083\n",
            "Epoch 665/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 44.7380 - mse: 44.7380\n",
            "Epoch 666/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.6756 - mse: 44.6756\n",
            "Epoch 667/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 44.6123 - mse: 44.6123\n",
            "Epoch 668/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 44.5387 - mse: 44.5387\n",
            "Epoch 669/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 44.4717 - mse: 44.4717\n",
            "Epoch 670/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.4095 - mse: 44.4095\n",
            "Epoch 671/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.3450 - mse: 44.3450\n",
            "Epoch 672/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.2802 - mse: 44.2802\n",
            "Epoch 673/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.2123 - mse: 44.2123\n",
            "Epoch 674/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.1446 - mse: 44.1446\n",
            "Epoch 675/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.0759 - mse: 44.0759\n",
            "Epoch 676/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 44.0092 - mse: 44.0092\n",
            "Epoch 677/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.9350 - mse: 43.9350\n",
            "Epoch 678/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.8565 - mse: 43.8565\n",
            "Epoch 679/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.7844 - mse: 43.7844\n",
            "Epoch 680/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 43.7115 - mse: 43.7115\n",
            "Epoch 681/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 43.6308 - mse: 43.6308\n",
            "Epoch 682/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.5565 - mse: 43.5565\n",
            "Epoch 683/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.4857 - mse: 43.4857\n",
            "Epoch 684/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.4227 - mse: 43.4227\n",
            "Epoch 685/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.3572 - mse: 43.3572\n",
            "Epoch 686/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.2831 - mse: 43.2831\n",
            "Epoch 687/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.2166 - mse: 43.2166\n",
            "Epoch 688/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.1484 - mse: 43.1484\n",
            "Epoch 689/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.0838 - mse: 43.0838\n",
            "Epoch 690/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 43.0173 - mse: 43.0173\n",
            "Epoch 691/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 42.9493 - mse: 42.9493\n",
            "Epoch 692/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 42.8869 - mse: 42.8869\n",
            "Epoch 693/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 42.8242 - mse: 42.8242\n",
            "Epoch 694/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 42.7589 - mse: 42.7589\n",
            "Epoch 695/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 42.6974 - mse: 42.6974\n",
            "Epoch 696/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 42.6365 - mse: 42.6365\n",
            "Epoch 697/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 42.5820 - mse: 42.5820\n",
            "Epoch 698/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 42.5284 - mse: 42.5284\n",
            "Epoch 699/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 42.4722 - mse: 42.4722\n",
            "Epoch 700/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 42.4119 - mse: 42.4119\n",
            "Epoch 701/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 42.3483 - mse: 42.3483\n",
            "Epoch 702/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 42.2909 - mse: 42.2909\n",
            "Epoch 703/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 42.2309 - mse: 42.2309\n",
            "Epoch 704/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 42.1668 - mse: 42.1668\n",
            "Epoch 705/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 42.1025 - mse: 42.1025\n",
            "Epoch 706/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 42.0438 - mse: 42.0438\n",
            "Epoch 707/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.9884 - mse: 41.9884\n",
            "Epoch 708/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.9393 - mse: 41.9393\n",
            "Epoch 709/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 41.8892 - mse: 41.8892\n",
            "Epoch 710/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 41.8357 - mse: 41.8357\n",
            "Epoch 711/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 41.7774 - mse: 41.7774\n",
            "Epoch 712/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 41.7235 - mse: 41.7235\n",
            "Epoch 713/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.6540 - mse: 41.6540\n",
            "Epoch 714/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.5879 - mse: 41.5879\n",
            "Epoch 715/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.5337 - mse: 41.5337\n",
            "Epoch 716/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.4732 - mse: 41.4732\n",
            "Epoch 717/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.4126 - mse: 41.4126\n",
            "Epoch 718/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.3562 - mse: 41.3562\n",
            "Epoch 719/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.3062 - mse: 41.3062\n",
            "Epoch 720/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.2542 - mse: 41.2542\n",
            "Epoch 721/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 41.2039 - mse: 41.2039\n",
            "Epoch 722/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.1507 - mse: 41.1507\n",
            "Epoch 723/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 41.0945 - mse: 41.0945\n",
            "Epoch 724/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 41.0376 - mse: 41.0376\n",
            "Epoch 725/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.9872 - mse: 40.9872\n",
            "Epoch 726/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.9314 - mse: 40.9314\n",
            "Epoch 727/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.8809 - mse: 40.8809\n",
            "Epoch 728/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 40.8295 - mse: 40.8295\n",
            "Epoch 729/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.7801 - mse: 40.7801\n",
            "Epoch 730/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.7243 - mse: 40.7243\n",
            "Epoch 731/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.6745 - mse: 40.6745\n",
            "Epoch 732/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.6256 - mse: 40.6256\n",
            "Epoch 733/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.5761 - mse: 40.5761\n",
            "Epoch 734/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.5304 - mse: 40.5304\n",
            "Epoch 735/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 40.4639 - mse: 40.4639\n",
            "Epoch 736/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.4111 - mse: 40.4111\n",
            "Epoch 737/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 40.3569 - mse: 40.3569\n",
            "Epoch 738/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 40.2990 - mse: 40.2990\n",
            "Epoch 739/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.2450 - mse: 40.2451\n",
            "Epoch 740/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.1944 - mse: 40.1944\n",
            "Epoch 741/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.1353 - mse: 40.1353\n",
            "Epoch 742/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.0801 - mse: 40.0801\n",
            "Epoch 743/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 40.0282 - mse: 40.0282\n",
            "Epoch 744/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 39.9809 - mse: 39.9809\n",
            "Epoch 745/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.9279 - mse: 39.9279\n",
            "Epoch 746/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 39.8767 - mse: 39.8767\n",
            "Epoch 747/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.8252 - mse: 39.8252\n",
            "Epoch 748/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.7742 - mse: 39.7742\n",
            "Epoch 749/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.7244 - mse: 39.7243\n",
            "Epoch 750/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.6784 - mse: 39.6784\n",
            "Epoch 751/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.6301 - mse: 39.6301\n",
            "Epoch 752/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.5880 - mse: 39.5880\n",
            "Epoch 753/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.5378 - mse: 39.5378\n",
            "Epoch 754/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.4899 - mse: 39.4899\n",
            "Epoch 755/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.4410 - mse: 39.4410\n",
            "Epoch 756/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.3952 - mse: 39.3952\n",
            "Epoch 757/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.3477 - mse: 39.3477\n",
            "Epoch 758/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 39.2968 - mse: 39.2968\n",
            "Epoch 759/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 39.2527 - mse: 39.2527\n",
            "Epoch 760/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.2104 - mse: 39.2104\n",
            "Epoch 761/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.1745 - mse: 39.1745\n",
            "Epoch 762/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.1275 - mse: 39.1275\n",
            "Epoch 763/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 39.0710 - mse: 39.0710\n",
            "Epoch 764/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 39.0193 - mse: 39.0193\n",
            "Epoch 765/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.9709 - mse: 38.9709\n",
            "Epoch 766/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 38.9207 - mse: 38.9207\n",
            "Epoch 767/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.8694 - mse: 38.8694\n",
            "Epoch 768/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 38.8229 - mse: 38.8229\n",
            "Epoch 769/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 38.7763 - mse: 38.7763\n",
            "Epoch 770/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.7360 - mse: 38.7360\n",
            "Epoch 771/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.6931 - mse: 38.6931\n",
            "Epoch 772/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.6496 - mse: 38.6496\n",
            "Epoch 773/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.6084 - mse: 38.6084\n",
            "Epoch 774/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 38.5646 - mse: 38.5646\n",
            "Epoch 775/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.5212 - mse: 38.5212\n",
            "Epoch 776/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.4773 - mse: 38.4773\n",
            "Epoch 777/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.4354 - mse: 38.4354\n",
            "Epoch 778/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.3840 - mse: 38.3840\n",
            "Epoch 779/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.3351 - mse: 38.3351\n",
            "Epoch 780/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.2844 - mse: 38.2844\n",
            "Epoch 781/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.2399 - mse: 38.2399\n",
            "Epoch 782/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.1970 - mse: 38.1970\n",
            "Epoch 783/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.1554 - mse: 38.1554\n",
            "Epoch 784/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.1143 - mse: 38.1143\n",
            "Epoch 785/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.0697 - mse: 38.0697\n",
            "Epoch 786/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 38.0248 - mse: 38.0248\n",
            "Epoch 787/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.9808 - mse: 37.9808\n",
            "Epoch 788/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.9254 - mse: 37.9254\n",
            "Epoch 789/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.8827 - mse: 37.8827\n",
            "Epoch 790/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.8381 - mse: 37.8381\n",
            "Epoch 791/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 37.7927 - mse: 37.7927\n",
            "Epoch 792/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.7527 - mse: 37.7527\n",
            "Epoch 793/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 37.7119 - mse: 37.7119\n",
            "Epoch 794/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 37.6678 - mse: 37.6678\n",
            "Epoch 795/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.6320 - mse: 37.6320\n",
            "Epoch 796/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.5993 - mse: 37.5993\n",
            "Epoch 797/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.5602 - mse: 37.5602\n",
            "Epoch 798/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.5138 - mse: 37.5138\n",
            "Epoch 799/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.4676 - mse: 37.4676\n",
            "Epoch 800/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 37.4252 - mse: 37.4252\n",
            "Epoch 801/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.3882 - mse: 37.3882\n",
            "Epoch 802/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.3551 - mse: 37.3551\n",
            "Epoch 803/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.3094 - mse: 37.3094\n",
            "Epoch 804/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 37.2689 - mse: 37.2689\n",
            "Epoch 805/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 37.2312 - mse: 37.2312\n",
            "Epoch 806/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.1935 - mse: 37.1935\n",
            "Epoch 807/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.1455 - mse: 37.1455\n",
            "Epoch 808/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.1047 - mse: 37.1047\n",
            "Epoch 809/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.0634 - mse: 37.0634\n",
            "Epoch 810/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 37.0206 - mse: 37.0206\n",
            "Epoch 811/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.9754 - mse: 36.9754\n",
            "Epoch 812/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 36.9363 - mse: 36.9363\n",
            "Epoch 813/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.8980 - mse: 36.8980\n",
            "Epoch 814/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.8627 - mse: 36.8627\n",
            "Epoch 815/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.8290 - mse: 36.8290\n",
            "Epoch 816/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.7943 - mse: 36.7943\n",
            "Epoch 817/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.7631 - mse: 36.7631\n",
            "Epoch 818/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.7270 - mse: 36.7270\n",
            "Epoch 819/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.6959 - mse: 36.6959\n",
            "Epoch 820/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.6633 - mse: 36.6633\n",
            "Epoch 821/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.6254 - mse: 36.6254\n",
            "Epoch 822/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.5905 - mse: 36.5905\n",
            "Epoch 823/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.5508 - mse: 36.5508\n",
            "Epoch 824/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.5132 - mse: 36.5132\n",
            "Epoch 825/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.4701 - mse: 36.4701\n",
            "Epoch 826/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.4303 - mse: 36.4303\n",
            "Epoch 827/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.3944 - mse: 36.3944\n",
            "Epoch 828/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.3535 - mse: 36.3535\n",
            "Epoch 829/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.3078 - mse: 36.3078\n",
            "Epoch 830/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.2701 - mse: 36.2701\n",
            "Epoch 831/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.2324 - mse: 36.2324\n",
            "Epoch 832/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.1928 - mse: 36.1928\n",
            "Epoch 833/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.1523 - mse: 36.1523\n",
            "Epoch 834/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.1153 - mse: 36.1153\n",
            "Epoch 835/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.0815 - mse: 36.0815\n",
            "Epoch 836/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 36.0458 - mse: 36.0458\n",
            "Epoch 837/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 36.0084 - mse: 36.0084\n",
            "Epoch 838/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 35.9660 - mse: 35.9660\n",
            "Epoch 839/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.9334 - mse: 35.9334\n",
            "Epoch 840/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.8991 - mse: 35.8991\n",
            "Epoch 841/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.8737 - mse: 35.8737\n",
            "Epoch 842/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.8344 - mse: 35.8344\n",
            "Epoch 843/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.7992 - mse: 35.7992\n",
            "Epoch 844/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.7635 - mse: 35.7635\n",
            "Epoch 845/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.7315 - mse: 35.7315\n",
            "Epoch 846/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.6943 - mse: 35.6943\n",
            "Epoch 847/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.6623 - mse: 35.6623\n",
            "Epoch 848/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.6261 - mse: 35.6261\n",
            "Epoch 849/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.5919 - mse: 35.5919\n",
            "Epoch 850/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.5537 - mse: 35.5537\n",
            "Epoch 851/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.5209 - mse: 35.5209\n",
            "Epoch 852/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 35.4870 - mse: 35.4870\n",
            "Epoch 853/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.4587 - mse: 35.4587\n",
            "Epoch 854/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.4277 - mse: 35.4277\n",
            "Epoch 855/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.3941 - mse: 35.3941\n",
            "Epoch 856/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.3630 - mse: 35.3630\n",
            "Epoch 857/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 35.3318 - mse: 35.3318\n",
            "Epoch 858/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.2981 - mse: 35.2981\n",
            "Epoch 859/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.2656 - mse: 35.2656\n",
            "Epoch 860/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.2358 - mse: 35.2358\n",
            "Epoch 861/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 35.2110 - mse: 35.2110\n",
            "Epoch 862/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 35.1849 - mse: 35.1849\n",
            "Epoch 863/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.1608 - mse: 35.1608\n",
            "Epoch 864/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.1304 - mse: 35.1304\n",
            "Epoch 865/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.1030 - mse: 35.1030\n",
            "Epoch 866/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.0717 - mse: 35.0717\n",
            "Epoch 867/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.0370 - mse: 35.0370\n",
            "Epoch 868/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 35.0095 - mse: 35.0095\n",
            "Epoch 869/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.9823 - mse: 34.9823\n",
            "Epoch 870/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.9516 - mse: 34.9516\n",
            "Epoch 871/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 34.9238 - mse: 34.9238\n",
            "Epoch 872/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 34.8907 - mse: 34.8907\n",
            "Epoch 873/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.8575 - mse: 34.8575\n",
            "Epoch 874/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 34.8268 - mse: 34.8268\n",
            "Epoch 875/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.8004 - mse: 34.8004\n",
            "Epoch 876/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.7687 - mse: 34.7687\n",
            "Epoch 877/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.7391 - mse: 34.7391\n",
            "Epoch 878/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.7025 - mse: 34.7025\n",
            "Epoch 879/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.6707 - mse: 34.6707\n",
            "Epoch 880/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.6337 - mse: 34.6338\n",
            "Epoch 881/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 34.6005 - mse: 34.6005\n",
            "Epoch 882/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.5655 - mse: 34.5655\n",
            "Epoch 883/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 34.5261 - mse: 34.5261\n",
            "Epoch 884/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.4964 - mse: 34.4964\n",
            "Epoch 885/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.4656 - mse: 34.4656\n",
            "Epoch 886/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.4356 - mse: 34.4356\n",
            "Epoch 887/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.4080 - mse: 34.4080\n",
            "Epoch 888/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.3764 - mse: 34.3764\n",
            "Epoch 889/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 34.3518 - mse: 34.3518\n",
            "Epoch 890/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.3227 - mse: 34.3227\n",
            "Epoch 891/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.2968 - mse: 34.2968\n",
            "Epoch 892/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 34.2678 - mse: 34.2678\n",
            "Epoch 893/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.2425 - mse: 34.2425\n",
            "Epoch 894/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.2167 - mse: 34.2167\n",
            "Epoch 895/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.1938 - mse: 34.1938\n",
            "Epoch 896/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 34.1816 - mse: 34.1816\n",
            "Epoch 897/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 34.1655 - mse: 34.1655\n",
            "Epoch 898/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.1443 - mse: 34.1443\n",
            "Epoch 899/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 34.1224 - mse: 34.1224\n",
            "Epoch 900/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 34.1065 - mse: 34.1065\n",
            "Epoch 901/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 34.0792 - mse: 34.0792\n",
            "Epoch 902/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 34.0550 - mse: 34.0550\n",
            "Epoch 903/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.0258 - mse: 34.0258\n",
            "Epoch 904/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 34.0019 - mse: 34.0019\n",
            "Epoch 905/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.9788 - mse: 33.9788\n",
            "Epoch 906/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 33.9477 - mse: 33.9477\n",
            "Epoch 907/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.9253 - mse: 33.9253\n",
            "Epoch 908/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 33.8929 - mse: 33.8929\n",
            "Epoch 909/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.8643 - mse: 33.8643\n",
            "Epoch 910/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.8398 - mse: 33.8398\n",
            "Epoch 911/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.8126 - mse: 33.8126\n",
            "Epoch 912/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.7847 - mse: 33.7847\n",
            "Epoch 913/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.7591 - mse: 33.7591\n",
            "Epoch 914/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.7367 - mse: 33.7367\n",
            "Epoch 915/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 33.7125 - mse: 33.7125\n",
            "Epoch 916/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.6878 - mse: 33.6878\n",
            "Epoch 917/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.6585 - mse: 33.6585\n",
            "Epoch 918/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.6291 - mse: 33.6291\n",
            "Epoch 919/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.6069 - mse: 33.6069\n",
            "Epoch 920/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.5808 - mse: 33.5808\n",
            "Epoch 921/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.5606 - mse: 33.5606\n",
            "Epoch 922/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.5412 - mse: 33.5412\n",
            "Epoch 923/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.5221 - mse: 33.5221\n",
            "Epoch 924/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.5038 - mse: 33.5038\n",
            "Epoch 925/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.4846 - mse: 33.4846\n",
            "Epoch 926/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.4636 - mse: 33.4636\n",
            "Epoch 927/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.4393 - mse: 33.4393\n",
            "Epoch 928/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.4107 - mse: 33.4107\n",
            "Epoch 929/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.3851 - mse: 33.3851\n",
            "Epoch 930/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.3601 - mse: 33.3601\n",
            "Epoch 931/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.3394 - mse: 33.3394\n",
            "Epoch 932/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.3190 - mse: 33.3190\n",
            "Epoch 933/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.2965 - mse: 33.2965\n",
            "Epoch 934/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.2759 - mse: 33.2759\n",
            "Epoch 935/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.2520 - mse: 33.2520\n",
            "Epoch 936/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.2309 - mse: 33.2309\n",
            "Epoch 937/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 33.2107 - mse: 33.2107\n",
            "Epoch 938/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.1902 - mse: 33.1902\n",
            "Epoch 939/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.1670 - mse: 33.1670\n",
            "Epoch 940/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.1506 - mse: 33.1507\n",
            "Epoch 941/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.1378 - mse: 33.1378\n",
            "Epoch 942/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.1204 - mse: 33.1204\n",
            "Epoch 943/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.1001 - mse: 33.1001\n",
            "Epoch 944/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.0816 - mse: 33.0816\n",
            "Epoch 945/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.0689 - mse: 33.0689\n",
            "Epoch 946/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 33.0526 - mse: 33.0526\n",
            "Epoch 947/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.0357 - mse: 33.0357\n",
            "Epoch 948/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 33.0144 - mse: 33.0144\n",
            "Epoch 949/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.9945 - mse: 32.9945\n",
            "Epoch 950/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.9747 - mse: 32.9747\n",
            "Epoch 951/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.9486 - mse: 32.9486\n",
            "Epoch 952/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.9322 - mse: 32.9322\n",
            "Epoch 953/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.9102 - mse: 32.9102\n",
            "Epoch 954/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.8918 - mse: 32.8918\n",
            "Epoch 955/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.8733 - mse: 32.8733\n",
            "Epoch 956/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.8565 - mse: 32.8565\n",
            "Epoch 957/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.8380 - mse: 32.8380\n",
            "Epoch 958/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.8169 - mse: 32.8169\n",
            "Epoch 959/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.7928 - mse: 32.7928\n",
            "Epoch 960/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.7753 - mse: 32.7753\n",
            "Epoch 961/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.7550 - mse: 32.7550\n",
            "Epoch 962/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.7350 - mse: 32.7350\n",
            "Epoch 963/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.7180 - mse: 32.7180\n",
            "Epoch 964/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 32.7062 - mse: 32.7062\n",
            "Epoch 965/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.6910 - mse: 32.6910\n",
            "Epoch 966/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.6758 - mse: 32.6758\n",
            "Epoch 967/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.6585 - mse: 32.6585\n",
            "Epoch 968/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.6473 - mse: 32.6473\n",
            "Epoch 969/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 32.6284 - mse: 32.6284\n",
            "Epoch 970/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.6237 - mse: 32.6237\n",
            "Epoch 971/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 32.6094 - mse: 32.6094\n",
            "Epoch 972/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.5899 - mse: 32.5899\n",
            "Epoch 973/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 32.5752 - mse: 32.5752\n",
            "Epoch 974/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.5629 - mse: 32.5629\n",
            "Epoch 975/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 32.5459 - mse: 32.5459\n",
            "Epoch 976/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.5325 - mse: 32.5325\n",
            "Epoch 977/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.5161 - mse: 32.5161\n",
            "Epoch 978/1000\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 32.4968 - mse: 32.4968\n",
            "Epoch 979/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.4779 - mse: 32.4779\n",
            "Epoch 980/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.4649 - mse: 32.4649\n",
            "Epoch 981/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.4478 - mse: 32.4478\n",
            "Epoch 982/1000\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 32.4332 - mse: 32.4332\n",
            "Epoch 983/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.4113 - mse: 32.4113\n",
            "Epoch 984/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 32.3929 - mse: 32.3929\n",
            "Epoch 985/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.3762 - mse: 32.3762\n",
            "Epoch 986/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 32.3619 - mse: 32.3619\n",
            "Epoch 987/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.3448 - mse: 32.3448\n",
            "Epoch 988/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.3344 - mse: 32.3344\n",
            "Epoch 989/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.3230 - mse: 32.3230\n",
            "Epoch 990/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.3145 - mse: 32.3145\n",
            "Epoch 991/1000\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 32.2938 - mse: 32.2938\n",
            "Epoch 992/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.2786 - mse: 32.2786\n",
            "Epoch 993/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.2626 - mse: 32.2626\n",
            "Epoch 994/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.2423 - mse: 32.2423\n",
            "Epoch 995/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.2272 - mse: 32.2272\n",
            "Epoch 996/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.2133 - mse: 32.2133\n",
            "Epoch 997/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.1993 - mse: 32.1993\n",
            "Epoch 998/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.1892 - mse: 32.1892\n",
            "Epoch 999/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.1727 - mse: 32.1727\n",
            "Epoch 1000/1000\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 32.1547 - mse: 32.1547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0e7f5c390>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(scaled_features)\n",
        "bostonDF['KERAS_PREDICTED_PRICE_BATCH'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "VphOOB8Tiju6",
        "outputId": "c3c59cbe-2fb6-4de9-c6c6-cecb71ace4ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5af30308-77ca-47c4-8b5d-499a6ab75f4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE_SGD</th>\n",
              "      <th>PREDICTED_PRICE_BATCH_RANDOM</th>\n",
              "      <th>PREDICTED_PRICE_BATCH</th>\n",
              "      <th>KERAS_PREDICTED_PRICE_BATCH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.439906</td>\n",
              "      <td>26.048340</td>\n",
              "      <td>27.394484</td>\n",
              "      <td>27.751959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>24.394043</td>\n",
              "      <td>24.028449</td>\n",
              "      <td>24.236665</td>\n",
              "      <td>24.930891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>28.954733</td>\n",
              "      <td>28.567896</td>\n",
              "      <td>30.969979</td>\n",
              "      <td>30.904530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>28.680936</td>\n",
              "      <td>28.283078</td>\n",
              "      <td>30.668465</td>\n",
              "      <td>30.651634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>28.351467</td>\n",
              "      <td>27.972934</td>\n",
              "      <td>30.033437</td>\n",
              "      <td>30.067129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>25.840615</td>\n",
              "      <td>25.447954</td>\n",
              "      <td>26.542167</td>\n",
              "      <td>27.000416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.752975</td>\n",
              "      <td>21.402594</td>\n",
              "      <td>20.313115</td>\n",
              "      <td>21.446548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>19.904206</td>\n",
              "      <td>19.603360</td>\n",
              "      <td>17.241789</td>\n",
              "      <td>18.673147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>14.097085</td>\n",
              "      <td>13.860908</td>\n",
              "      <td>8.354012</td>\n",
              "      <td>10.743446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>20.043619</td>\n",
              "      <td>19.725452</td>\n",
              "      <td>17.586879</td>\n",
              "      <td>18.999100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5af30308-77ca-47c4-8b5d-499a6ab75f4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5af30308-77ca-47c4-8b5d-499a6ab75f4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5af30308-77ca-47c4-8b5d-499a6ab75f4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      CRIM    ZN  ...  PREDICTED_PRICE_BATCH  KERAS_PREDICTED_PRICE_BATCH\n",
              "0  0.00632  18.0  ...              27.394484                    27.751959\n",
              "1  0.02731   0.0  ...              24.236665                    24.930891\n",
              "2  0.02729   0.0  ...              30.969979                    30.904530\n",
              "3  0.03237   0.0  ...              30.668465                    30.651634\n",
              "4  0.06905   0.0  ...              30.033437                    30.067129\n",
              "5  0.02985   0.0  ...              26.542167                    27.000416\n",
              "6  0.08829  12.5  ...              20.313115                    21.446548\n",
              "7  0.14455  12.5  ...              17.241789                    18.673147\n",
              "8  0.21124  12.5  ...               8.354012                    10.743446\n",
              "9  0.17004  12.5  ...              17.586879                    18.999100\n",
              "\n",
              "[10 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "IdsPhy2hiju6"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}